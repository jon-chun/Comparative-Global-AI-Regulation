\documentclass[a4paper,12pt]{article}

% ──────────────────────────────────────────────
% Font — Times New Roman (JCP / Taylor & Francis)
% ──────────────────────────────────────────────
\usepackage[T1]{fontenc}
\usepackage{mathptmx}   % Times-compatible body text and maths

% ──────────────────────────────────────────────
% Page layout — A4, 2.5 cm margins, double spacing
% ──────────────────────────────────────────────
\usepackage[a4paper, margin=2.5cm]{geometry}
\usepackage[doublespacing]{setspace}
\usepackage[parfill]{parskip}

% ──────────────────────────────────────────────
% Section headings — bold, serif (Times)
% ──────────────────────────────────────────────
\usepackage{titlesec}
\titleformat{\section}{\Large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\large\bfseries}{\thesubsection}{1em}{}
\titleformat{\subsubsection}{\normalsize\bfseries}{\thesubsubsection}{1em}{}

% ──────────────────────────────────────────────
% Notes — Chatham House numbered endnotes
% ──────────────────────────────────────────────
\usepackage{endnotes}
\let\footnote=\endnote
\renewcommand{\notesname}{Notes}

% ──────────────────────────────────────────────
% Bibliography — biblatex verbose-note for footnote-style citations
% ──────────────────────────────────────────────
\usepackage[hyphens]{url}
\usepackage{csquotes}
\usepackage[
  style=verbose-note,
  backend=biber,
  abbreviate=true,
  ibidtracker=false
]{biblatex}
\addbibresource{jcp2026_cyber-governance-trilemma.bib}

% Remap non-standard entry types to @misc
\DeclareSourcemap{
  \maps[datatype=bibtex]{
    \map{
      \step[typesource=legal, typetarget=misc]
    }
    \map{
      \step[typesource=legislation, typetarget=misc]
    }
    \map{
      \step[typesource=report, typetarget=misc]
    }
  }
}

% ──────────────────────────────────────────────
% Figures and tables
% ──────────────────────────────────────────────
\usepackage{graphicx}
\usepackage{float}
\usepackage{placeins}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{longtable}
\usepackage{array}
\usepackage{adjustbox}

% ──────────────────────────────────────────────
% Hyperlinks and URLs
% ──────────────────────────────────────────────
\usepackage{xcolor}
\usepackage[breaklinks,colorlinks,linkcolor=blue!70!black,citecolor=blue!70!black,urlcolor=blue!70!black]{hyperref}
\usepackage{bookmark}

% ──────────────────────────────────────────────
% Miscellaneous
% ──────────────────────────────────────────────
\usepackage{enumitem}

\DeclareUnicodeCharacter{2060}{}

% ──────────────────────────────────────────────
% Metadata
% ──────────────────────────────────────────────
\title{\textbf{The Cyber Governance Trilemma:\\Comparative AI Regulation in the EU, China,\\and the United States}}

\author{
    Jon Chun\textsuperscript{1} \\
    Christian Schroeder de Witt\textsuperscript{2} \\
    Katherine Elkins\textsuperscript{1} \\
    \\
    \textsuperscript{1}Kenyon College \\
    \textsuperscript{2}University of Oxford
}

\date{}  % JCP provides its own date in production

% ══════════════════════════════════════════════
\begin{document}
% ══════════════════════════════════════════════

\maketitle

% ──────────────────────────────────────────────
\begin{abstract}
Artificial intelligence regulation now poses one of the central challenges of international cyber governance. As AI systems are deployed across critical infrastructure, cybersecurity operations, and information networks, the frameworks governing their development shape global cyber policy in ways that few anticipated a decade ago. This paper presents a comparative analysis of the three most influential AI regulatory approaches: the EU AI Act, China's sector-specific regulations, and the US approach spanning Executive Order~\#14110, its revocation under the Trump administration, and state-level initiatives including California's SB~53. Drawing on regulatory competition theory, a governance typology grounded in comparative regulatory capitalism, and the concept of regime complexity, we argue that these three jurisdictions face a \textit{cyber governance trilemma}: no jurisdiction can simultaneously optimise innovation speed, safety and rights protection, and regulatory interoperability. The EU accepts slower innovation in exchange for robust safety guarantees. The US, especially after revoking EO~\#14110, has moved in the opposite direction, favouring speed over regulatory consistency. China pursues state-directed competitiveness but at the price of international interoperability. This regulatory fragmentation is itself a cyber risk: it creates compliance gaps that sophisticated threat actors can exploit and complicates international norm-building. Incorporating developments through late 2025, we conclude with implications for international cyber governance, including lessons for developing nations navigating between competing regulatory models.
\end{abstract}

\medskip
\noindent\textbf{Keywords:} AI regulation, cyber governance, regulatory fragmentation, EU AI Act, comparative policy, Brussels Effect

% ──────────────────────────────────────────────
\section{Introduction}
% ──────────────────────────────────────────────

The regulation of artificial intelligence can no longer be separated from cyber governance. AI-generated deepfakes undermine election integrity and enable sophisticated social engineering attacks. Autonomous agents probe critical infrastructure for vulnerabilities at machine speed, while adversarial actors exploit large language models to generate polymorphic malware that evades signature-based defences.\autocite{kello2017virtual} AI systems have also become integral to cyber defence, from anomaly detection in financial networks to automated threat intelligence. How a jurisdiction regulates AI therefore determines the safety of its own digital infrastructure and the security posture of every interconnected system.\autocite{nye2014regime}

As jurisdictions race to regulate these dual-use capabilities, the resulting patchwork of divergent frameworks creates a new form of systemic cyber risk: regulatory fragmentation. Cybersecurity governance once centred on technical standards and incident response.\autocite{broeders2017public} The challenge now goes further: how should societies develop, deploy, and constrain AI? The divergent answers from the EU, China, and the United States have implications both for domestic technology policy and for the architecture of international cyber governance. Gaps between regulatory regimes create seams that sophisticated threat actors can exploit: an AI system that satisfies Chinese content-labelling requirements may violate EU transparency standards, while a system compliant with the EU AI Act's high-risk provisions may be entirely unregulated under the post-2025 US framework.

A growing body of scholarship has examined how different jurisdictions are approaching AI governance. Early comparative work mapped the ethical principles underpinning national AI strategies in the US, EU, and UK,\autocite{cath2018artificial}\autocite{floridi2018ai4people} while Stix identified three distinct policy pathways (ethics-based, rights-based, and risk-based) that governments have pursued.\autocite{stix2021actionable} Smuha documented the emergence of `regulatory competition' in AI, arguing that jurisdictions now race to regulate AI as much as to develop it.\autocite{smuha2021race} Studies have examined individual regimes in depth, including the EU AI Act,\autocite{veale2021demystifying} China's layered regulatory approach,\autocite{roberts2021chinese} and the broader framing contests that shape AI policy.\autocite{ulnicane2021framing} Yet few studies offer a systematic three-way comparison that situates the EU, China, and the US within a unified analytical framework, and fewer still examine AI regulation through the lens of cyber governance. This paper addresses that gap.

The regulatory terrain has shifted considerably since 2024. The EU AI Act's prohibitions on high-risk AI practices took effect in February 2025. In the United States, President Trump revoked Executive Order~\#14110 on his first day in office in January 2025, while California enacted a successor transparency law (SB~53) in September 2025 and New York passed the RAISE Act in December 2025. China amended its Cybersecurity Law in October 2025 to incorporate AI governance provisions for the first time in national legislation. AI regulation is evolving rapidly and unevenly, with consequences for the coherence of international cyber governance that are only beginning to be understood.

The paper makes three contributions. First, it provides a systematic comparative analysis of AI regulatory frameworks across the three largest AI economies, updated through late 2025. Second, it introduces a \textit{cyber governance trilemma}: the proposition that jurisdictions cannot simultaneously optimise innovation speed, safety and rights protection, and regulatory interoperability. Third, it draws out implications for international cyber governance, including for developing nations caught between competing regulatory models.

% ──────────────────────────────────────────────
\section{Theoretical Framework}
% ──────────────────────────────────────────────

Comparative studies of technology regulation have long observed that jurisdictions adopt divergent governance strategies, shaped by institutional traditions, political economy, and geopolitical ambition.\autocite{cath2018artificial}\autocite{ulnicane2021framing} As Smuha argues, the global `race to AI regulation' now rivals the race to develop the technology itself.\autocite{smuha2021race} We draw on four theoretical perspectives to move beyond description.

First, we draw on \textit{regulatory competition theory}, which holds that jurisdictions compete, and learn from one another, through their regulatory choices.\autocite{bradford_brussels_2020} Bradford's concept of the `Brussels Effect' describes a mechanism by which the EU's large single market incentivises global firms to adopt EU standards unilaterally. We examine whether the EU AI Act exhibits this dynamic and how competing regulatory models may limit it.

Second, we employ a \textit{governance typology} that distinguishes regulatory frameworks along two dimensions: (1)~the degree of \textit{centralisation} (top-down versus distributed) and (2)~the primary \textit{regulatory modality} (ex ante risk-based rules versus ex post enforcement through existing legal frameworks). This typology draws on Majone's analysis of the `regulatory state' in Europe\autocite{majone1994rise} and Levi-Faur's account of the `global diffusion of regulatory capitalism',\autocite{levifaur2005global} applying their comparative frameworks to the AI domain. Applied to AI, the typology places the EU squarely in the centralised, ex ante quadrant, while the US leans towards distributed, ex post enforcement. China combines elements of both: centralised guidance paired with decentralised, sector-specific implementation.

Third, we attend to the \textit{innovation--safety tradeoff} that pervades AI governance debates globally.\autocite{stix2021actionable} Each jurisdiction resolves this tension differently, reflecting deeper differences in political values. The EU privileges precaution and fundamental rights;\autocite{veale2021demystifying} China couples social stability concerns with state-directed industrial competitiveness;\autocite{roberts2021chinese} in the US, market-driven innovation tends to win out, with safety addressed through voluntary commitments.

Fourth, we draw on Nye's concept of \textit{regime complexity}\autocite{nye2014regime} and the broader literature on internet governance fragmentation\autocite{mueller2017internet}\autocite{denardis2014global} to situate AI regulation within the evolving architecture of cyber governance. The absence of a single overarching regime for AI governance mirrors the fragmented regime complex that characterises cyberspace more broadly, and it complicates international cooperation, norm-building, and the management of transboundary cyber risks.\autocite{eichensehr2019cyber}

% ──────────────────────────────────────────────
\section{Methodology}
% ──────────────────────────────────────────────

This study employs a comparative case study methodology to examine how AI regulatory frameworks interact with, and reshape, cyber governance across three jurisdictions: the European Union, the People's Republic of China, and the United States (at both federal and state levels). Case selection follows a most-different-systems design: the three cases represent the world's largest AI economies while instantiating distinct governance traditions: centralised ex ante regulation, hybrid state-guided governance, and distributed market-oriented oversight. This design allows identification of underlying factors that drive regulatory divergence and its cyber governance consequences.

Our analytical approach combines \textit{legal and policy document analysis} with \textit{regime complexity mapping}. For each jurisdiction, we examined the principal legislative texts, executive orders, regulatory guidance, and enforcement actions current through late 2025, with particular attention to provisions governing AI systems in cybersecurity-relevant domains: critical infrastructure, defence and intelligence, surveillance, and information integrity. We supplement primary legal sources with peer-reviewed scholarship, policy institute reports, and practitioner commentary. The comparison is structured around the governance typology and regime complexity frameworks introduced above, with the comparative regulatory timeline (Section~4) serving as an original empirical anchor for tracking the pace, sequencing, and competitive dynamics of regulatory activity across jurisdictions.

% ──────────────────────────────────────────────
\section{Comparative Regulatory Timeline}
% ──────────────────────────────────────────────

Table~\ref{tab:timeline} presents a comparative regulatory timeline constructed as a structured dataset of AI governance milestones across the three jurisdictions. Inclusion criteria were: (1)~binding legislation, executive orders, or regulations with direct AI governance provisions; (2)~institutional actions establishing new AI-specific governance bodies; and (3)~enforcement milestones marking the operational commencement of regulatory obligations. Non-binding strategies, voluntary commitments, and subnational actions below the state or provincial level were excluded unless they demonstrably shaped national regulatory trajectories (as California's SB~1047 debate did for US AI governance).

\begin{table}[ht]
\centering\small
\begin{tabularx}{\linewidth}{@{}l l X@{}}
\toprule
\textbf{Date} & \textbf{Jurisdiction} & \textbf{Milestone} \\
\midrule
Nov 2016 & China & Cybersecurity Law adopted (effective June 2017) \\
Apr 2018 & EU & AI Strategy: \textit{Artificial Intelligence for Europe} \\
Feb 2019 & US & Executive Order 13859 on maintaining US leadership in AI \\
Apr 2021 & EU & AI Act proposed by European Commission \\
Sept 2021 & China & Data Security Law takes effect \\
Nov 2021 & China & Personal Information Protection Law (PIPL) takes effect \\
Mar 2022 & China & Algorithm Recommendation Regulation takes effect \\
Oct 2022 & US & Blueprint for an AI Bill of Rights \\
Jan 2023 & China & Deep Synthesis Regulation takes effect \\
Aug 2023 & China & Generative AI Measures take effect \\
Oct 2023 & US & Biden signs EO~\#14110 (100+ delegated tasks to 50+ agencies) \\
Dec 2023 & EU & AI Act provisionally agreed (European Parliament + Council) \\
Jan 2024 & US & NIST establishes US AI Safety Institute \\
Aug 2024 & EU & AI Act enters into force \\
Sept 2024 & US & Governor Newsom vetoes California SB~1047 \\
Jan 2025 & US & Trump revokes EO~\#14110; signs EO~14179 (3 days later) \\
Feb 2025 & EU & Prohibited AI practices take effect \\
Aug 2025 & EU & GPAI model governance obligations take effect \\
Aug 2025 & China & State Council issues `AI Plus' Action Plan \\
Sept 2025 & US & California enacts SB~53 (Transparency in Frontier AI Act) \\
Oct 2025 & China & NPC amends Cybersecurity Law to incorporate AI governance \\
Dec 2025 & US & New York enacts RAISE Act \\
Aug 2026 & EU & High-risk AI system requirements take effect (scheduled) \\
\bottomrule
\end{tabularx}
\caption{Comparative timeline of major AI regulatory milestones (2016--2026).}
\label{tab:timeline}
\end{table}

Three patterns stand out. Most striking is the acceleration: only three entries fall before 2021, while fifteen fall between 2023 and 2026, a compression that tracks the post-ChatGPT urgency across all three jurisdictions. Equally notable is divergent sequencing. China led with targeted, sector-specific regulations (algorithmic recommendation, deepfakes, generative AI) before attempting consolidation through Cybersecurity Law amendments in 2025. The EU pursued a single comprehensive legislative instrument over a multi-year process, accepting slower adoption in exchange for systemic coherence. The US oscillated between executive action and state-level experimentation, with the revocation of EO~\#14110 representing a dramatic reversal in federal-level ambition. Finally, the timeline shows competitive clustering: between 2023 and 2025, all three jurisdictions undertook major regulatory actions within months of each other, consistent with the regulatory competition dynamics that Smuha theorised.\autocite{smuha2021race} This uncoordinated proliferation compounds the interoperability challenges that the trilemma describes.

% ──────────────────────────────────────────────
\section{The European Union}
% ──────────────────────────────────────────────

\subsection{Overview}

The 2024 EU AI Act is widely described as the world's first comprehensive AI law.\autocite{eu_ai_act_2024} Just as prior European general-purpose legislation, such as the 2016 General Data Protection Regulation (GDPR),\autocite{gdpr_article7} the EU AI Act represents complex joint efforts across various EU bodies, including the European Commission, the European Parliament, and the European Council. Influence on the Act's formation was taken publicly by national government officials, such as France's Macron lobbying for exemptions for open-source AI providers such as Mistral,\autocite{abboud_eus_2023} as well as by lobbying and industry groups, including Big Tech\autocite{perrigo_exclusive_2023} and German pro-open-source non-profit LAION.\autocite{noauthor_call_nodate}

Originally constructed within a product safety framework, the legislation was later blended with a fundamental rights agenda at the behest of the European Parliament.\autocite{caroli_podcast} This syncretism sets the EU AI Act apart from prior European legislation that built on established frameworks such as the GDPR. It also follows earlier regulatory initiatives, including the 2022 Digital Markets Act\autocite{dma} and, of direct relevance to cyber governance, the NIS2 Directive on cybersecurity.\autocite{eu_nis2_2022} Together, the AI Act and NIS2 form the most tightly integrated AI-cybersecurity governance framework among the three jurisdictions examined here. The NIS2 Directive, which entered into force in January 2023 and required member-state transposition by October 2024, designates operators in energy, transport, banking, health, digital infrastructure, and public administration as `essential entities' subject to mandatory cybersecurity risk management and incident reporting. Where AI systems are deployed within these sectors (as they increasingly are, for grid management, fraud detection, medical diagnostics, and network traffic analysis) providers must satisfy both the AI Act's risk-based requirements and NIS2's cybersecurity obligations simultaneously. This regulatory overlap is by design: it reflects the EU's systemic approach to digital governance, in which AI safety, data protection (GDPR), and cybersecurity (NIS2) form an interlocking regulatory architecture.

\subsection{The Geopolitics of the Act}

Besides regulating the EU single market, the Act is often regarded as a strategic effort by the European Commission to establish themselves as the leading AI rulemakers globally.\autocite{noauthor_eus_2024} The logic is that companies across the world will prioritise compliance with European AI law out of economic necessity, not through coercion: the `Brussels Effect'.\autocite{bradford_brussels_2020}\autocite{almada_brussels_2024} One direct institutional consequence is the establishment of the EU AI Office, which will oversee AI regulation, provide a central pool of AI expertise to member states, and support a `strategic, coherent, and effective European approach on AI at the international level'.\autocite{eu_ai_office}

\subsection{Risk Classification and Regulation}

Designed as \textit{adaptive legislation}, the AI Act leaves details intentionally open for later specification. Its centrepiece is a risk classification system that places obligations primarily on the developers (`providers') of AI systems. Crucially, the legislation applies both to systems placed on the EU market and to AI systems whose output is used in the EU, giving it considerable extraterritorial reach.

Four risk tiers structure the framework. \textit{Prohibited practices} include AI systems used for social scoring, manipulative subliminal techniques, and most real-time remote biometric identification.\autocite[Article 5]{eu_ai_act_2024} \textit{High-risk systems} constitute the majority of regulation, requiring risk management systems, data governance, human oversight, and cybersecurity measures.\autocite[Article 6]{eu_ai_act_2024} \textit{Limited-risk} systems face transparency obligations, while \textit{minimal-risk} systems are left unregulated.

From a cyber governance standpoint, the high-risk category is the most consequential. AI systems used as safety components of critical infrastructure (Annex~III, Area~2), AI systems used for law enforcement and border control (Annex~III, Areas~6--7), and AI systems deployed in democratic processes all fall within this tier. Article~15 of the Act explicitly requires providers of high-risk AI systems to achieve an `appropriate level of cybersecurity' proportionate to the risks, including resilience against adversarial manipulation, data poisoning, and model extraction attacks, requirements that mirror and reinforce NIS2 obligations for essential entities. No other jurisdiction has linked AI regulation to cybersecurity standards this explicitly.

A separate category addresses \textit{general-purpose AI (GPAI) models}, with heightened obligations for those posing `systemic risk', defined as models trained with cumulative compute exceeding $10^{25}$ floating-point operations.\autocite{eu_ai_act_2024} Providers must register with the European Commission and adhere to wide-ranging safety and security criteria, including adversarial testing and model evaluation.

\subsection{Open Source, Innovation, and Implementation}

Wide-ranging exemptions exist for providers of AI systems under free and open-source software licences,\autocite[Article 53--54]{eu_ai_act_2024} provided they do not contain GPAI models of systemic risk. However, these exemptions are narrower than the de facto permissive environment in both the US and China, and the tension between protecting open-source development and regulating systemic risk remains unresolved.

By late 2025, the AI Act was no longer aspirational; it was being enforced. The prohibited-practices provisions, banning social scoring, manipulative subliminal techniques, and most real-time remote biometric identification, became legally binding on 2~February 2025. GPAI model governance obligations followed on 2~August 2025, with penalties reaching \texteuro35~million or 7\% of global annual turnover for non-compliance. Full high-risk AI system requirements are scheduled for August 2026. In parallel, NIS2 transposition deadlines drove member states to integrate AI-specific cybersecurity obligations into national frameworks, producing a synchronised rollout of AI safety and cybersecurity governance that neither the US nor China has attempted.

% ──────────────────────────────────────────────
\section{China}
% ──────────────────────────────────────────────

\subsection{Overview}

China's approach to AI governance is a hybrid: centralised strategic direction combined with decentralised, sector-specific implementation and selective enforcement. The central government sets overarching objectives, above all social stability and technological self-sufficiency, while provincial governments and sectoral regulators translate these into operational rules, often with considerable latitude for local experimentation.\autocite{zhang2022} The design is a calculated attempt to take the best of both the EU model (coherent top-down guidance) and the US model (market-driven innovation) while avoiding the perceived weaknesses of each.

What distinguishes China from a cyber governance perspective is its institutional architecture. The Cyberspace Administration of China (CAC) is simultaneously the country's internet regulator, data protection authority, and primary AI governance body. This consolidation is deliberate: it ensures that AI regulation operates within, and reinforces, China's broader cyber sovereignty strategy, in which data governance, content control, algorithmic regulation, and cybersecurity operate as a single regulatory apparatus rather than separate policy domains.

\subsection{Laws and Regulations}

\FloatBarrier

China has advanced some of the first AI laws and regulations at the national level, summarised in Table~\ref{tab:china_laws}. Unlike the horizontal risk-based approach of the EU, China has favoured the sector-specific US approach of laws tailored to specific use cases. Despite appearances of centralised government control, Chinese AI regulations are the product of an iterative process involving diverse stakeholders including mid-level bureaucrats, academics, corporations, startups, and think tanks.\autocite{sheehan2024} The central government relies upon a pipeline of experts to formulate and interpret the details, while local officials mainly ensure alignment with Chinese and socialist ideology.\autocite{zhang2022}

\begin{table}[ht]
\centering\small
\begin{tabularx}{\linewidth}{@{}l >{\raggedright\arraybackslash}p{2.4cm} >{\raggedright\arraybackslash}p{1.8cm} >{\raggedright\arraybackslash}p{3.6cm} >{\raggedright\arraybackslash}p{3.6cm}@{}}
\toprule
\textbf{Date} & \textbf{Title} & \textbf{Issuing Body} & \textbf{Description} & \textbf{Cyber Governance Dimension} \\
\midrule
June 2017 & Cybersecurity Law & NPC & Legal frameworks for cybersecurity, data protection, and network security. & Foundation of China's cyber governance architecture; amended Oct 2025 to incorporate AI. \\
\addlinespace
Sept.\ 2021 & Data Security Law & NPC & Regulations on data processing and security affecting AI systems. & Data localisation and cross-border transfer controls; shapes AI training data governance. \\
\addlinespace
Nov.\ 2021 & PIPL & NPC & Comprehensive data privacy law governing personal information. & Requires consent for AI processing of personal data; extraterritorial provisions. \\
\addlinespace
Mar.\ 2022 & Algorithm Recommendation Reg. & CAC & Regulates recommendation algorithms, requiring transparency and fairness. & First algorithm-specific governance globally; algorithm registration as cyber governance tool. \\
\addlinespace
Jan.\ 2023 & Deep Synthesis Reg. & CAC & Governs generative AI authenticity and traceability. & Content-labelling mandates for AI-generated material; anti-disinformation function. \\
\addlinespace
Aug.\ 2023 & Generative AI Measures & CAC + 6 & Obligations on generative AI providers for legality and cybersecurity. & Mandatory security assessments; bridges AI governance and network security enforcement. \\
\bottomrule
\end{tabularx}
\caption{Chinese AI laws and regulations, with cyber governance dimensions.}
\label{tab:china_laws}
\end{table}

\FloatBarrier

\subsection{Registration, Compliance, and Industrial Policy}

On paper, China's AI regulation requirements are the most burdensome of the three jurisdictions. Model registration, data management rules, and ongoing compliance monitoring illustrate how strict central regulation can slow innovation. As of March 2024, only 546 AI models had been registered, and just seventy were Large Language Models\autocite{chinamoney2024}, in stark contrast to the over 500,000 open-source LLMs on Huggingface.co,\autocite{huggingface2024} which is banned in China.\autocite{chinatalk2023}

Yet in practice, enforcement is selective by design. China's `Made In China 2025' industrial policy supports 10,000 `Little Giants', small and mid-sized enterprises recognised as key sources of innovation,\autocite{globaltimes2021} which are informally afforded regulatory leeway.\autocite{zhang2024} Startups and SMEs largely operate beneath the enforcement threshold as long as they lack a large public presence.\autocite{zhang2022} Selective enforcement is also accompanied by a growing network of municipal AI regulatory sandboxes. Shanghai and Shenzhen have both established AI pilot zones that allow companies to test novel AI applications under relaxed compliance requirements, mirroring the EU AI Act's sandbox provisions but with characteristically less formal structure. The Shanghai AI pilot zone, for instance, permits experimentation with autonomous driving, medical AI diagnostics, and smart-city applications under streamlined registration procedures, providing a controlled pathway for innovation that the national regulatory framework does not yet accommodate.

China's selective approach also has an international dimension. Through the Belt and Road Initiative's digital infrastructure investments, Chinese AI companies, and the regulatory norms embedded in their systems, are being exported to Southeast Asia, Central Asia, and Africa. Smart-city platforms built by Huawei and Alibaba, surveillance systems incorporating Chinese AI, and telecommunications infrastructure carrying Chinese technical standards extend China's cyber governance model far beyond its borders.\autocite{roberts2021chinese} The outcome is a de facto `Beijing Effect' in AI governance. Developing nations that adopt Chinese-built AI infrastructure implicitly adopt the data governance and content-control frameworks embedded within it, often without the conscious legislative adoption that characterises the Brussels Effect.

The foundational layer of this regulatory apparatus predates AI-specific regulation by several years. The 2017 Cybersecurity Law, the 2021 Data Security Law, and the Personal Information Protection Law (PIPL) established a comprehensive data governance architecture, covering collection, storage, cross-border transfer, and security, within which AI-specific regulations now operate.\autocite{roberts2021chinese} The CAC's algorithm registration system, which requires companies to disclose the logic and parameters of recommendation algorithms, doubles as an AI governance mechanism and a tool for ensuring that algorithmic systems do not undermine social stability or state information-control objectives. International firms that comply with China's AI regulations must therefore also comply with its broader cybersecurity and data-localisation requirements, a coupling that neither the EU nor the US imposes with comparable force.

\subsection{Recent Developments (2025)}

Two developments in 2025 tightened the link between AI governance and China's cybersecurity architecture. In August, the State Council issued the `AI Plus' Action Plan,\autocite{china_aiplus_2025} an industrial policy targeting 70\% AI penetration across key economic sectors by 2027, framing AI adoption as a matter of national competitiveness. More significant for cyber governance, the National People's Congress amended the Cybersecurity Law in October\autocite{china_cybersecurity_2025} to incorporate AI governance provisions for the first time in primary national legislation. The amendments mandate AI ethics review and risk assessment for systems deployed in critical infrastructure, increase administrative penalties tenfold (from RMB~1~million to RMB~10~million), and grant the CAC expanded enforcement authority over AI systems that process personal data or affect network security. Mandatory AI content-labelling rules also took effect in September 2025. The institutional logic is plain: AI governance is an extension of the cybersecurity and data-sovereignty framework that the CAC already administers, not a separate policy domain.

% ──────────────────────────────────────────────
\section{United States}
% ──────────────────────────────────────────────

\subsection{Overview}

On 30 October 2023, President Biden signed Executive Order~\#14110 on the `Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence'.\autocite{whitehouse2023b} The order delegated AI responsibilities to more than fifty existing federal agencies with more than one hundred specific tasks, moving beyond the voluntary commitments secured in July 2023\autocite{whitehouse2023a} and the October 2022 AI Bill of Rights.\autocite{whitehouse2022}

\subsection{Laws and Regulations}

The US regulatory tradition emphasises decentralised oversight: Congress passes framework legislation, and specialised federal agencies enforce rules within their domains.\autocite{nsf2024}\autocite{sec2024}\autocite{epa2024} Technology regulation has historically combined legislative action, executive orders, agency rulemaking, and industry self-regulation.\autocite{cusumano2021}\autocite{minow2023} This distributed approach reflects both philosophical distrust of centralised power and the practical influence of a \$46~billion lobbying industry.\autocite{massoglia2024} EO~\#14110 was somewhat exceptional in that the executive branch initiated AI-related policies partly because it could respond more quickly than Congress, but the order nonetheless reflected the US instinct for delegating specific objectives to agencies with pre-existing domain expertise.\autocite{perkins2024}

\subsection{Executive Order 14110}

\FloatBarrier

The order's directives addressed eight policy areas including safety and security, innovation and competition, worker support, bias and civil rights, consumer protection, privacy, federal use of AI, and international leadership.\autocite{whitehouse2024a} Both the 180-day and 270-day deadlines were met.\autocite{whitehouse2024b}\autocite{whitehouse2024d}

EO~\#14110 addressed many of the same concerns as the EU AI Act, but through different institutional means.\autocite{crs2024} Where the EU established a new centralised authority (the AI Office), the US augmented existing federal agencies. The US approach was more immediately actionable but also more vulnerable to political reversal, as January 2025 would demonstrate. Within this structure, the National Institute of Standards and Technology (NIST) established the US AI Safety Institute in January 2024,\autocite{nist2024a} with task forces on safety, evaluation, measurement, and risk management.\autocite{nist2024b}

\subsection{AI and Cybersecurity Governance}

Unlike the EU's integrated approach through the AI Act and NIS2, the United States lacks a unified framework connecting AI governance with cybersecurity policy. Before its revocation, EO~\#14110 assigned cybersecurity-related AI tasks to multiple agencies. NIST was directed to develop AI red-teaming standards and adversarial testing guidelines. The Department of Homeland Security (DHS) received responsibility for assessing AI-related threats to critical infrastructure, while the Cybersecurity and Infrastructure Security Agency (CISA) published its `Roadmap for Artificial Intelligence' in late 2023, identifying AI as both an essential tool for cyber defence and a vector for novel threats. Meanwhile, the Department of Defence's Replicator initiative committed to fielding autonomous AI-enabled systems at scale, raising governance questions about autonomous cyber operations that no civilian regulatory framework addresses.

Spread across NIST, CISA, DHS, DOD, and sector-specific agencies, this distributed approach produced rapid progress under EO~\#14110's coordination mandate. But it also created a structural vulnerability: because coordination depended on presidential direction rather than legislative authority, the entire framework was susceptible to political reversal.

\subsubsection{Revocation and Its Cyber Governance Consequences}

It was realised on 20~January 2025, when President Trump revoked EO~\#14110 on his first day in office. His replacement order, EO~14179 (`Removing Barriers to American Leadership in Artificial Intelligence'),\autocite{trump2025eo14179} establishes a policy of sustaining US AI dominance through economic competitiveness and national security, but contains no cybersecurity requirements, inter-agency coordination mandates, safety benchmarks, or compliance deadlines. The 180-day AI Action Plan it mandates had, as of late 2025, not addressed the cybersecurity dimensions that EO~\#14110 had begun to operationalise.

The consequences for US cyber governance are immediate. CISA's AI Roadmap lacks binding authority without executive-level backing, and NIST's AI Safety Institute continues to operate but under a narrowed mandate. What remains is a governance vacuum: AI-cybersecurity coordination now depends on agency initiative rather than presidential direction, even as AI-enabled cyber threats proliferate. By prioritising innovation speed through deregulation, the Trump administration has weakened the coherence of federal AI-cybersecurity governance, a concrete expression of the US position in the trilemma.

\subsection{California SB~1047 and State-Level Regulation}

\FloatBarrier

California Senate Bill 1047, introduced in February 2024, attempted to establish a regulatory framework for `frontier models' defined by computational resources.\autocite{LegiScan2024} The bill attracted both broad support and vigorous opposition.\autocite{thehill_ai_supporters_2024}\autocite{nunez2024} Governor Newsom vetoed SB~1047 on 29 September 2024\autocite{newsom2024veto} because he opposed standards solely based on model size, favouring instead assessment of deployment contexts. The episode illustrates the gravitational pull of institutional traditions; even ambitious ex ante proposals get reshaped by the US system's preference for ex post, risk-based approaches.

The SB~1047 debate catalysed a wave of state-level AI legislation in 2025. California enacted SB~53, the Transparency in Frontier Artificial Intelligence Act,\autocite{casb53_2025} on 29 September 2025. SB~53 shifts from prescriptive safety mandates to a transparency-centred approach: developers of frontier models (trained at $>10^{26}$ operations) must publish annual risk-governance frameworks, issue pre-deployment transparency reports, and report critical safety incidents within 15 days. New York followed with the RAISE Act\autocite{ny_raise_2025} in December 2025, requiring annual independent audits and 72-hour incident reporting. Together, these laws suggest that US AI governance is settling on transparency and disclosure obligations rather than ex ante safety requirements.

% ──────────────────────────────────────────────
\section{Discussion: The Cyber Governance Trilemma}
% ──────────────────────────────────────────────

\subsection{Comparative Synthesis}

When the governance typology from Section~2 is applied to the three cases, the jurisdictions sort cleanly. The EU sits in the centralised, ex ante quadrant: the AI Act establishes a unified, risk-based framework that regulates AI systems \textit{before} they reach the market, enforced through a new centralised authority. At the opposite corner, the US relies on distributed, ex post oversight; even before the revocation of EO~\#14110, the federal approach depended heavily on industry self-regulation and existing legal frameworks. China falls between the two: centralised guidance and registration requirements coexist with decentralised, sector-specific enforcement and calculated regulatory forbearance for innovative SMEs.

These divergent positions produce what we term the \textit{cyber governance trilemma}. Borrowing from the international political economy literature on `impossible trinities', we argue that jurisdictions cannot simultaneously optimise three objectives:

\begin{enumerate}
  \item \textbf{Innovation speed}: minimising regulatory barriers to AI development and deployment.
  \item \textbf{Safety and rights protection}: ensuring AI systems do not cause harm, discrimination, or rights violations.
  \item \textbf{Regulatory interoperability}: maintaining compatibility with other jurisdictions' frameworks to enable cross-border operations.
\end{enumerate}

This is not coincidence; it reflects a structural incompatibility among the three objectives, operating through three bilateral tensions. The logic parallels Rodrik's globalisation trilemma, which holds that nations cannot simultaneously maintain deep economic integration, national sovereignty, and democratic politics.\autocite{rodrik2011globalization} Rodrik showed that the constraints are distributional: the gains from integration come at the cost of policy autonomy or democratic accountability. The AI governance trilemma works similarly. It is driven by distributional choices about who bears risk, who captures economic value, and who sets international norms, and each objective requires institutional commitments that undermine the other two.

Consider first the \textit{innovation--safety tension}, the most familiar of the three. Stringent ex ante regulation requires developers to demonstrate compliance before deployment, adding time and cost that slow innovation. The EU's conformity assessment regime for high-risk AI systems is a case in point: providers must complete risk assessments, establish quality management systems, and maintain technical documentation before gaining market access, a process that delays deployment relative to jurisdictions without such requirements. Conversely, permissive regulation accelerates deployment but increases harm probability, as the US experience with largely unregulated AI-enabled content recommendation systems and deepfake generation tools demonstrates.

The \textit{safety--interoperability tension} gets less attention but binds just as tightly. Stronger domestic safety standards inevitably diverge from other jurisdictions' definitions of acceptable risk, impeding cross-border AI operations. China's content-labelling requirements for AI-generated material, for example, are fundamentally incompatible with EU transparency standards, which focus on system-level risk classification rather than content-level marking. As a jurisdiction specifies safety requirements more rigorously, and as those requirements come to reflect domestic political values, divergence from jurisdictions with different risk tolerances widens.

A third edge of the triangle, the \textit{innovation--interoperability tension}, operates through the Brussels Effect mechanism itself. Establishing global regulatory standards requires comprehensive, prescriptive rules with extraterritorial reach; but such rules constrain domestic innovators who must comply with standards designed for international projection. The EU's GPAI provisions have drawn criticism on exactly this point: they impose compliance burdens on European open-source AI developers that their US and Chinese competitors do not face, potentially deterring the very innovation that global standard-setting requires.

At bottom, the trilemma reflects the fact that AI governance involves distributional choices (who bears risk, who captures economic value, who sets international norms) that map onto incompatible political economy preferences across jurisdictions. No single regulatory design can simultaneously minimise barriers to innovation, maximise safety guarantees, and maintain cross-border compatibility, because each objective demands institutional choices that work against the other two. Each jurisdiction therefore sacrifices one objective, as illustrated in Figure~\ref{fig:trilemma} and summarised in Table~\ref{tab:trilemma}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/trilemma_triangle.pdf}
    \caption{The cyber governance trilemma. Each jurisdiction is positioned near the two objectives it prioritises, sacrificing the third. The three edges represent the bilateral tensions that drive the structural trade-off.}
    \label{fig:trilemma}
\end{figure} Safety and interoperability are the EU's twin priorities, pursued through Brussels Effect ambitions but at the cost of innovation velocity. The US, particularly after the revocation of EO~\#14110, has made the opposite bet: innovation speed first, with safety guarantees left largely to market discipline and state-level experimentation. China charts a third course, coupling rapid AI development with selective safety enforcement while embedding the entire apparatus within a cyber sovereignty strategy that is inherently incompatible with multi-stakeholder governance models.\autocite{mueller2017internet}

\begin{table}[ht]
\centering\small
\begin{tabularx}{\linewidth}{@{}l c c c@{}}
\toprule
\textbf{Jurisdiction} & \textbf{Innovation} & \textbf{Safety/Rights} & \textbf{Interoperability} \\
\midrule
European Union & Constrained & Strong (ex ante) & High (Brussels Effect) \\
United States & Prioritised & Weak (post-EO revocation) & Moderate \\
China & Prioritised (selective) & Selective enforcement & Low (cyber sovereignty) \\
\bottomrule
\end{tabularx}
\caption{The cyber governance trilemma: how each jurisdiction resolves the innovation--safety--interoperability trade-off.}
\label{tab:trilemma}
\end{table}

On the Brussels Effect thesis, our analysis offers only partial support. Extraterritorial reach does create compliance incentives for global firms, but the simultaneous emergence of substantive frameworks in both the US and China limits the unilateral standard-setting power that the EU enjoyed with the GDPR. AI governance is evolving towards a \textit{multipolar regulatory order}\autocite{smuha2021race} in which firms must navigate competing compliance regimes, a pattern that mirrors the broader fragmentation of internet governance.\autocite{nye2014regime}

\subsection{Implications for International Cyber Governance}

Regulatory fragmentation carries concrete implications for the international cyber governance community.

The most immediate is that divergence itself becomes a source of cyber risk. Divergent AI governance frameworks create compliance gaps, seams between regulatory regimes that well-resourced threat actors can exploit. Where AI systems in critical infrastructure must satisfy different safety standards depending on jurisdiction, the weakest link determines overall cyber resilience. The EU's pairing of the AI Act with the NIS2 Directive illustrates one model of integrated AI-cybersecurity governance, but no comparable integration exists at the international level.

These gaps are widening. The Trump administration's revocation of EO~\#14110 opened a regulatory gulf between the US and EU at the moment the AI Act entered enforcement. European firms operating in the US face declining regulatory certainty, while US firms must still comply with the AI Act for EU market access. Meanwhile, China's integration of AI governance into its Cybersecurity Law creates a self-contained regulatory system that reinforces cyber sovereignty, complicating already-fraught US--China technology competition and multilateral cyber norm processes.

The ripple effects extend well beyond the three major jurisdictions. Developing countries that lack the institutional capacity to design bespoke AI governance frameworks face a choice between regulatory models that carries geopolitical weight.\autocite{denardis2014global}\autocite{carr2016public} The GDPR experience, in which more than 130 countries adopted data protection laws influenced by the EU model, suggests that many countries will align with one of the three major AI regulatory frameworks. Early evidence confirms this expectation but also reveals more complex dynamics than simple emulation.

Brazil's AI regulatory framework, approved by the Senate in December 2024, draws heavily on the EU AI Act's risk-based classification system while incorporating provisions for algorithmic impact assessments tailored to the country's acute concerns about racial and socioeconomic discrimination. India's proposed Digital India Act takes a different path, emphasising light-touch regulation and industry self-governance that more closely resembles the US approach, reflecting India's ambition to position itself as a global AI development hub. Singapore's Model AI Governance Framework, now in its third iteration, charts a pragmatic middle course: voluntary guidelines grounded in risk assessment and transparency principles, designed to attract international AI investment without imposing compliance burdens that would deter firms from establishing regional headquarters.

At the multilateral level, the African Union's Continental AI Strategy (2024) and the ASEAN Guide on AI Governance and Ethics both signal a preference for principles-based frameworks that preserve regulatory flexibility; but the practical governance choices of individual member states are increasingly shaped by which major power provides their AI infrastructure. Developing nations' choices about AI regulation determine their alignment in international cyber norm debates, including their positions in the UN Open-Ended Working Group on Information and Communications Technologies and the Global Digital Compact. The trilemma therefore operates across the entire global governance architecture, as regulatory choices cascade through investment relationships, infrastructure dependencies, and multilateral alignments.

All of this complicates efforts to establish international AI governance norms through the United Nations and other multilateral forums. If the three largest AI powers cannot agree on basic regulatory approaches, whether ex ante or ex post, centralised or distributed, rights-based or innovation-first, the prospects for a binding international regime remain limited. More realistic pathways may include issue-specific agreements on autonomous weapons, deepfakes, or AI-enabled cyber operations; mutual recognition frameworks; or informal convergence driven by regulatory competition itself.

\subsection{AI Governance and Cyber Conflict}

The cyber governance trilemma extends beyond civilian AI regulation into the military and security domain, where the stakes are highest and the governance gaps widest. AI-enabled capabilities are transforming cyber operations across all three jurisdictions: automated vulnerability discovery, AI-generated spear-phishing at scale, autonomous network penetration tools, and machine-speed decision-making in cyber defence all blur the boundary between civilian AI governance and military cyber operations.\autocite{kello2017virtual}

Each jurisdiction handles this boundary differently, and none handles it well. In Europe, the AI Act explicitly exempts national security and defence applications from its scope, leaving military AI systems without a comparable governance framework at the EU level. China takes the opposite approach: its military-civil fusion strategy deliberately erases the distinction between civilian and military AI development, systematically channelling technologies developed under civilian regulations into People's Liberation Army modernisation programmes. The result is that China's ostensibly civilian AI governance framework has military implications that neither the EU nor the US accounts for. In the United States, programmes such as the Department of Defence's Replicator initiative and DARPA's AI-enabled cyber operations research are producing autonomous systems that occupy a governance grey zone between EO~14179's civilian innovation mandate and the Laws of Armed Conflict.

In the military domain, the trilemma bites hardest. No jurisdiction can simultaneously pursue AI-enabled military advantage (innovation), compliance with international humanitarian law and arms-control norms (safety), and interoperability with allies' autonomous systems (interoperability). The UN Group of Governmental Experts on Lethal Autonomous Weapons Systems and the Open-Ended Working Group on ICT security have both struggled to establish meaningful norms precisely because the three major AI powers approach military AI governance from incompatible positions. Issue-specific agreements (on autonomous cyber weapons, AI-generated disinformation in armed conflict, or machine-speed escalation dynamics) may represent the most achievable near-term pathway, but even these require a degree of regulatory convergence that the trilemma inherently impedes.

% ──────────────────────────────────────────────
\section{Conclusion}
% ──────────────────────────────────────────────

The EU, China, and the United States are constructing fundamentally different AI regulatory systems, each shaped by distinct institutional traditions, political values, and geopolitical ambitions. Regulatory fragmentation of this kind is a persistent feature of cyber governance, not a transitional inconvenience; it affects cybersecurity, international cooperation, and the distribution of technological power simultaneously. As AI capabilities advance and become more deeply woven into critical systems, the tensions will only sharpen. Whether jurisdictions can find workable compromises between innovation, safety, and interoperability will shape international cyber governance. There is little reason to expect convergence any time soon.

% ──────────────────────────────────────────────
\section*{Author Contributions}
% ──────────────────────────────────────────────
Jon Chun led the comparative analysis of the United States and China, the cyber governance trilemma framework, and the regulatory timeline dataset. Christian Schroeder de Witt led the analysis of the European Union and the regime complexity framing. Katherine Elkins contributed to the United States analysis and the developing-nations implications. All authors contributed to the theoretical framework, methodology, and comparative synthesis.

% ──────────────────────────────────────────────
\section*{About the Authors}
% ──────────────────────────────────────────────

\textbf{Jon Chun} is Visiting Instructor of Humanities at Kenyon College, with graduate degrees in computer science and electrical engineering from UC Berkeley and UT Austin and extensive industry experience in FinTech and cybersecurity. He is lead investigator for the Modern Language Association's participation in the NIST US AI Safety Institute and co-principal investigator on an IBM--Notre Dame Tech Ethics Lab grant examining AI decision-making in criminal justice contexts. His research encompasses human-centred AI, AI safety, and technology policy.

\textbf{Christian Schroeder de Witt} is Lecturer in the Department of Engineering Science at the University of Oxford and Principal Investigator of the Oxford Witt Lab for Trust in AI (OWL). A Royal Academy of Engineering Research Fellow and Schmidt Sciences AI2050 Fellow, his research addresses multi-agent security and AI assurance, examining how decentralised AI systems can be secured against emerging threats. He serves as an expert adviser to RAND, the BBC, and the UK government on AI governance and risk.

\textbf{Katherine Elkins} is Professor of Comparative Literature and Humanities at Kenyon College, with affiliated faculty status in Computing. She co-founded Kenyon's human-centred AI curriculum in 2016 and serves as co-principal investigator for both the NIST US AI Safety Institute and the IBM--Notre Dame Tech Ethics Lab grant on AI decision-making in recidivism cases. Her research bridges computational methods with humanistic inquiry to inform policy on AI safety, bias, and governance.

% ──────────────────────────────────────────────
% Endnotes — Chatham House numbered endnotes
% ──────────────────────────────────────────────
\FloatBarrier
\clearpage
\begingroup
\parindent 0pt
\parskip 0.5\baselineskip
\theendnotes
\endgroup

\end{document}
