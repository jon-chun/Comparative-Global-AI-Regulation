\documentclass[a4paper,11pt]{article}

% ──────────────────────────────────────────────
% Fonts — Source Sans Pro (sans-serif body)
% ──────────────────────────────────────────────
\usepackage[T1]{fontenc}
\usepackage{sourcesanspro}
\renewcommand{\familydefault}{\sfdefault}

% ──────────────────────────────────────────────
% Page layout — A4, 1-inch margins, 1.5 spacing
% ──────────────────────────────────────────────
\usepackage[a4paper, margin=1in]{geometry}
\usepackage[onehalfspacing]{setspace}
\usepackage[parfill]{parskip}          % no indent, paragraph skip

% ──────────────────────────────────────────────
% Section headings — numbered, bold, sans-serif
% ──────────────────────────────────────────────
\usepackage{titlesec}
\titleformat{\section}{\Large\bfseries\sffamily}{\thesection}{1em}{}
\titleformat{\subsection}{\large\bfseries\sffamily}{\thesubsection}{1em}{}
\titleformat{\subsubsection}{\normalsize\bfseries\sffamily}{\thesubsubsection}{1em}{}

% ──────────────────────────────────────────────
% Header / footer
% ──────────────────────────────────────────────
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\lhead{\small\sffamily Internet Policy Review --- Draft (convert to IPR template before submission)}
\rhead{\small\sffamily\thepage}
\renewcommand{\headrulewidth}{0.4pt}

% ──────────────────────────────────────────────
% Bibliography — biblatex + biber, APA 7th style
% natbib=true keeps \citep / \citet working
% ──────────────────────────────────────────────
\usepackage[hyphens]{url}   % must come before biblatex
\usepackage{csquotes}
\usepackage[
  style=apa,
  natbib=true,
  backend=biber,
  sorting=nyt,
  url=true,
  doi=true,
]{biblatex}
\addbibresource{ipr2026_comparative-global-ai-regulation.bib}

% Remap non-standard entry types to @misc
\DeclareSourcemap{
  \maps[datatype=bibtex]{
    \map{
      \step[typesource=legal, typetarget=misc]
    }
    \map{
      \step[typesource=legislation, typetarget=misc]
    }
    \map{
      \step[typesource=report, typetarget=misc]
    }
  }
}

% ──────────────────────────────────────────────
% Figures and tables
% ──────────────────────────────────────────────
\usepackage{graphicx}
\usepackage{float}
\usepackage{placeins}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{longtable}
\usepackage{array}
\usepackage{adjustbox}

% ──────────────────────────────────────────────
% Hyperlinks and URLs
% ──────────────────────────────────────────────
% url is already loaded by biblatex; just load hyperref and bookmark
\usepackage{xcolor}
\usepackage[breaklinks,colorlinks,linkcolor=blue!70!black,citecolor=blue!70!black,urlcolor=blue!70!black]{hyperref}
\usepackage{bookmark}

% ──────────────────────────────────────────────
% Miscellaneous
% ──────────────────────────────────────────────
\usepackage{enumitem}
\usepackage[blocks]{authblk}

\DeclareUnicodeCharacter{2060}{}

% ──────────────────────────────────────────────
% Metadata
% ──────────────────────────────────────────────
\title{\textbf{The EU AI Act in Global Context:\\
Comparing European, Chinese, and American AI Governance}}

\author[1]{Jon Chun\thanks{chunj@kenyon.edu}}
\author[2]{Christian Schroeder de Witt\thanks{cs@robots.ox.ac.uk}}
\author[1]{Katherine Elkins\thanks{elkinsk@kenyon.edu}}
\affil[1]{Kenyon College}
\affil[2]{University of Oxford}

\date{}  % IPR provides its own date in production

% ══════════════════════════════════════════════
\begin{document}
% ══════════════════════════════════════════════

\maketitle
\thispagestyle{fancy}

% ──────────────────────────────────────────────
% IPR front-matter
% ──────────────────────────────────────────────
\noindent\textbf{Teaser:} As the EU AI Act takes effect, its global influence hinges on whether rival Chinese and American frameworks will limit the Brussels Effect, with direct consequences for internet governance worldwide.

\medskip
\noindent\textbf{Keywords:} AI governance, EU AI Act, internet governance, Brussels Effect, regulatory competition

\medskip
\noindent\textbf{Funding statement:} This research received no external funding.

% ──────────────────────────────────────────────
\begin{abstract}
As the EU AI Act enters its enforcement phase, European policymakers face a question with broad implications for internet governance: will the Act reproduce the Brussels Effect that made the GDPR a global standard, or will competing frameworks in China and the United States limit the EU's regulatory influence? This paper develops a two-dimensional governance typology, mapping jurisdictions by degree of centralisation and regulatory modality (ex ante versus ex post), and applies it to the three most influential AI regulatory frameworks. We show that the EU occupies the centralised, ex ante quadrant. The US sits in the distributed, ex post corner. China pursues a hybrid combining top-down guidance with selective, sector-specific enforcement. Our analysis, current through late 2025, incorporates the Trump administration's revocation of Executive Order~\#14110, the AI Act's first prohibited-practices enforcement, and China's Cybersecurity Law amendments. Rather than a unilateral Brussels Effect, AI governance is evolving toward a multipolar regulatory landscape. Because each framework regulates algorithmic recommendation, content moderation, and platform accountability differently, these divergent approaches reshape internet governance as a whole. We conclude with concrete recommendations for European policymakers as the AI Act enters its implementation phase.
\end{abstract}

% ──────────────────────────────────────────────
\section{Introduction}
% ──────────────────────────────────────────────

On February 2, 2025, the EU AI Act's provisions banning prohibited AI practices became legally binding, marking the first operational enforcement of what is widely regarded as the world's most far-reaching AI law~\citep{eu_ai_act_2024}. Yet the Act does not operate in a regulatory vacuum. China has built a suite of sector-specific AI regulations since 2017 and amended its Cybersecurity Law in October 2025 to incorporate AI governance for the first time in national legislation~\citep{china_cybersecurity_2025}. The United States, meanwhile, moved in the opposite direction: the Trump administration revoked Executive Order~\#14110 on its first day in office in January 2025~\citep{trump2025eo14179}, dismantling the Biden-era coordination framework in favour of market-led development, even as California enacted a successor transparency law (SB~53) in September 2025~\citep{casb53_2025}. These developments raise a question of direct relevance to European policymakers: can the AI Act reproduce the ``Brussels Effect'' that made the GDPR a de facto global standard \citep{bradford_brussels_2020}, or will competing governance models limit Europe's regulatory reach?

A growing body of scholarship has examined these questions in isolation. Early comparative work mapped the ethical principles underpinning national AI strategies \citep{cath2018artificial,floridi2018ai4people}, while \citet{stix2021actionable} identified three distinct policy pathways (ethics-based, rights-based, and risk-based). \citet{smuha2021race} documented the emergence of ``regulatory competition'' in AI, and studies have examined individual regimes in depth, including the EU AI Act \citep{veale2021demystifying,novelli2024aiact}, China's layered approach \citep{roberts2021chinese,ding2024chinaai}, and the broader framing contests shaping AI policy \citep{ulnicane2021framing}. However, few studies offer a systematic three-way comparison within a unified analytical framework, and fewer still connect AI governance to internet regulation, despite the fact that AI-powered content moderation, recommendation algorithms, and platform decision-making sit at the intersection of both policy domains \citep{denardis2014,helberger2020}.

AI regulation intersects directly with internet governance. The EU AI Act operates alongside the Digital Services Act (DSA) and Digital Markets Act (DMA) as part of an integrated digital governance architecture. China's AI regulations are administered by the same Cyberspace Administration (CAC) that oversees internet content control. In the US, the absence of a federal equivalent to the DSA means that AI-driven content decisions remain largely self-regulated. How jurisdictions regulate AI therefore shapes platform accountability and algorithmic transparency, with downstream effects on online information ecosystems.

This paper addresses that gap. We develop a two-dimensional governance typology that maps jurisdictions by centralisation and regulatory modality, and apply it comparatively. Across the three jurisdictions, common concerns drive regulation: the societal risks of AI automation and disinformation \citep{UNAI2024}, the geopolitical stakes of AI as a dual-use technology \citep{itif2024chinaai}, and the tension between regulatory safety and innovation competitiveness \citep{suominen2020}. Each jurisdiction resolves these tensions differently. The EU pursues centralised, ex ante risk classification. The US distributes responsibilities across existing agencies with ex post enforcement. China combines top-down guidance with decentralised, sector-specific implementation. Rather than converging, these models are producing a multipolar regulatory order with concrete implications for European internet policy.

% ──────────────────────────────────────────────
\section{Theoretical Framework}
% ──────────────────────────────────────────────

Comparative studies of technology regulation have long observed that jurisdictions adopt divergent governance strategies shaped by institutional traditions and political economy \citep{cath2018artificial,ulnicane2021framing}. As \citet{smuha2021race} argues, the global ``race to AI regulation'' now rivals the race to develop the technology itself. This paper draws on three complementary theoretical perspectives to move beyond description toward analytical comparison of the EU, Chinese, and US approaches to AI governance, and to assess what those differences mean for European internet policy.

First, we draw on \textit{regulatory competition theory} as developed by \citet{bradford_brussels_2020} and situated within the broader literature on international regulatory dynamics \citep{drezner2007,simmons2001}. Bradford's ``Brussels Effect'' identifies two mechanisms by which the EU exports regulation: a \textit{de facto} effect, where multinational firms voluntarily adopt EU standards to serve the single market, and a \textit{de jure} effect, where third countries adopt EU-style rules to facilitate trade or fill domestic regulatory gaps. The GDPR exemplifies both mechanisms \citep{goddard2017eu}. For AI, however, three conditions that enabled the GDPR's influence---the EU's first-mover advantage, the absence of credible competing models, and the relatively low cost of compliance---may not hold for AI. China and the US have developed substantive AI governance frameworks of their own, and the compliance costs of the AI Act's risk classification may be considerably higher than the GDPR's notice-and-consent regime. We therefore treat the Brussels Effect not as a given but as a hypothesis to be tested against the comparative evidence.

Second, we develop a \textit{governance typology} that classifies regulatory frameworks along two dimensions: (1)~the degree of \textit{centralisation}, ranging from a single supranational authority to distributed agency-level oversight; and (2)~the primary \textit{regulatory modality}, ranging from ex ante rules that regulate AI systems before deployment to ex post enforcement that addresses harms after they occur. This two-dimensional space yields four ideal-type quadrants. We expect the EU to occupy the centralised--ex~ante quadrant, the US the distributed--ex~post quadrant, and China a hybrid position combining centralised strategic direction with decentralised, sector-specific implementation and selective enforcement. The typology builds on prior work distinguishing risk-based from rights-based governance \citep{stix2021actionable} and extends it by treating centralisation as an independent analytical dimension rather than conflating it with regulatory stringency.

Third, we attend to the \textit{innovation--safety tradeoff} that pervades AI governance debates globally. \citet{rodrik2011globalization} argues that economic governance faces a ``trilemma'' among deep integration, national sovereignty, and democratic politics---at most two can be fully achieved simultaneously. Applied to AI, jurisdictions face a three-way tension among regulatory safety, innovation competitiveness, and democratic legitimacy. The EU privileges precaution and fundamental rights \citep{veale2021demystifying}, accepting potential costs to innovation speed. China prioritises social stability alongside state-directed industrial competitiveness, sacrificing democratic input \citep{roberts2021chinese}. The US emphasises market-driven innovation, relying on voluntary commitments and ex post liability rather than democratic deliberation over ex ante rules. These different resolutions produce distinct internet governance outcomes. The EU layers AI regulation onto its DSA/DMA architecture. China governs AI and internet content through a single authority, the CAC. The US, by contrast, lacks an integrated framework linking AI governance to platform regulation \citep{denardis2014}.

By situating the empirical comparison within these frameworks, we aim to identify \textit{how} the three approaches differ, \textit{why} they differ, and what the implications are for the trajectory of European and global AI governance.

% ──────────────────────────────────────────────
\section{Methodology}
% ──────────────────────────────────────────────

This study employs a \textit{structured comparative analysis} of AI regulatory frameworks across three jurisdictions: the European Union, the People's Republic of China, and the United States (at both federal and state levels). The cases were selected because they represent the world's largest AI economies and because they instantiate distinct positions within the governance typology: centralised ex ante regulation (EU), hybrid state-guided governance (China), and distributed market-oriented regulation (US). Together, they encompass the principal models available to jurisdictions designing AI governance from scratch, making the comparison directly relevant to European policymakers assessing the AI Act's global positioning.

Our primary method is \textit{legal and policy document analysis}. For each jurisdiction we reviewed the principal legislative texts, executive orders, regulatory guidance, and official communications current through late 2025. We supplement legal analysis with peer-reviewed scholarship, policy institute reports, and expert commentary to contextualise enforcement and geopolitical significance. The comparison is organised around five analytical dimensions derived from the governance typology: (1)~degree of centralisation, (2)~regulatory modality (ex ante versus ex post), (3)~treatment of open-source and general-purpose AI, (4)~extraterritorial reach, and (5)~relationship to broader internet governance architectures. These dimensions structure the jurisdiction-level analysis and the comparative table in Appendix~A.

Our methodological contribution is framework-building, not empirical testing. The governance typology provides a tool for structured comparison that future work can extend through interviews with regulators, enforcement data, or compliance-cost metrics. Three limitations follow from this design. First, the study draws entirely on publicly available secondary sources. Second, the three-case design prioritises breadth over depth in any single jurisdiction. Third, AI regulation is evolving rapidly, so developments after late 2025 may alter the picture we describe.

% ──────────────────────────────────────────────
\section{The European Union}
% ──────────────────────────────────────────────

\subsection{Centralised Ex Ante Regulation}

In the governance typology, the EU AI Act exemplifies the centralised, ex ante quadrant. Rather than distributing oversight across existing agencies or waiting for harms to occur, the Act establishes a unified risk classification that regulates AI systems \textit{before} they reach the market, enforced through a new supranational body, the EU AI Office. This approach reflects a broader European regulatory tradition visible in the GDPR~\citep{gdpr_article7} and the DMA~\citep{dma}, but the AI Act is distinctive in blending product safety with fundamental rights, a combination forged during trilogue negotiations when the European Parliament insisted on rights-based provisions against Commission resistance~\citep{caroli_podcast}. The resulting legislation is, as MEP Dragos Tudorache put it, ``an opportunity to express our values''~\citep{tyrangiel_opinion_2024}.

Negotiated across the European Commission, Parliament, and Council, the Act attracted intense lobbying from national governments (notably France's advocacy for open-source exemptions~\citep{abboud_eus_2023}), Big Tech~\citep{perrigo_exclusive_2023}, and civil society groups. The final trilogue in December 2023 narrowed the Act's scope: national security, military, and research applications were excluded, and the Act was clarified not to apply outside European law~\citep{compromise}. Of greatest consequence, the emergence of ChatGPT in late 2022 forced policymakers to add a parallel governance track for general-purpose AI (GPAI) models~\citep[Article 3(63)]{eu_ai_act_2024}. This dual structure (risk classification for AI \textit{systems} and separate transparency obligations for GPAI \textit{models}) is unique among global AI regulations and reflects the centralised approach: a single legislative instrument governs both narrow-purpose and general-purpose AI, rather than delegating sector-specific rules to specialised agencies as China and the US do.

Beyond the AI Act itself, the EU has built a broader digital governance architecture. The DSA governs platform accountability and content moderation, the DMA addresses market dominance of digital gatekeepers, and the AI Act now adds risk-based requirements for AI systems deployed within these platforms. When an AI-powered recommendation algorithm operates on a ``very large online platform'' under the DSA, it must satisfy both the DSA's algorithmic transparency obligations and the AI Act's risk classification. This layered approach gives the EU the most integrated framework for governing the intersection of AI and internet policy, but it also creates compliance complexity with no parallel in either China or the US.

\subsection{The Brussels Effect and the AI Office}

Widely regarded as a strategic effort to position the EU as the global AI rulemaker~\citep{noauthor_eus_2024}, the Act has an explicit geopolitical dimension. The GDPR demonstrated both mechanisms of the Brussels Effect: de facto compliance by multinationals serving the single market, and de jure adoption by countries lacking their own frameworks (e.g., the Philippines' incorporation of the right to be forgotten)~\citep{almada_brussels_2024,philippines2012dataprivacy}. Whether the AI Act can replicate this dynamic is an open question, given the higher compliance costs and the existence of competing frameworks in China and the US. The institutional vehicle for this ambition is the EU AI Office, which combines domestic oversight with an explicit international mandate to pursue ``convergence toward common approaches''~\citep{eu_ai_office,eu_ai_roles}. In practice, the AI Office functions simultaneously as a centralised enforcement body and an instrument of European regulatory diplomacy.

\subsection{Risk Classification and the GPAI Track}

Designed as \textit{adaptive legislation}, the Act sets core obligations in the regulation while deliberately leaving technical details open for later specification as technology evolves. Obligations fall primarily on ``providers'' (developers), and the Act's extraterritorial reach extends to any AI system whose output is used within the EU~\citep[Article 2]{eu_ai_act_2024}, a provision with considerable implications for non-EU firms, particularly those in the US and China.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/eu_ai_act_decisiongram.pdf}
    \caption{Decision tree for providers of (GP)AI systems and GPAI models on the way to the EU market.}
    \label{fig:figure1}
\end{figure}

As shown in Figure~\ref{fig:figure2}, the risk classification system assigns AI systems to four tiers based on their deployment context: \textit{prohibited} practices (social scoring, manipulative subliminal techniques, most real-time remote biometric identification); \textit{high-risk} systems subject to conformity assessment, data governance, and post-market monitoring~\citep[Article 6, Annex I--III]{eu_ai_act_2024}; \textit{limited-risk} systems requiring only transparency obligations; and unregulated \textit{minimal-risk} systems. This ex ante classification is the Act's defining regulatory choice: risk is assessed before deployment based on intended use, not after harm materialises. This stands in sharp contrast to the US approach.

\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{figures/risk_pyramid.pdf}
    \caption{The risk pyramid for AI systems \citep[taken from][]{madiega2024artificial}.}
    \label{fig:figure2}
\end{figure}

For GPAI models, the Act creates a parallel governance track. Models whose training compute exceeds $10^{25}$ FLOPs are classified as posing ``systemic risk'' and face additional obligations including registration with the European Commission and thorough safety assessments~\citep[Article D]{eu_ai_act_2024}. The $10^{25}$ threshold was a political compromise between the Parliament and Commission~\citep{caroli_podcast}, and the Act deliberately leaves the criteria open for later revision. This compute-based threshold matters because it takes a fundamentally different approach to measuring risk than deployment-context assessment, the very distinction that led to the veto of California's SB~1047, as discussed below.

\subsection{Open Source and the Innovation Tension}

Open-source AI reveals a core tension in the EU's centralised approach. Open-source AI systems receive partial exemptions from compliance obligations~\citep[Article 53--54]{eu_ai_act_2024}, but these exemptions are narrower than they first appear. GPAI models classified as systemic risk receive no exemption regardless of licensing~\citep{eiras_near_2024,eiras_risks_2024}, and all open GPAI model providers must meet transparency requirements: publishing training data summaries~\citep[Article C(1)(d)]{eu_ai_act_2024} and respecting EU copyright law under Directive 2019/790~\citep{dsm_directive_2019,warso2024AIAct}. These obligations may prove onerous for community-developed models where no single entity controls the training pipeline, a scenario the Act does not clearly address.

This matters for the innovation--safety tradeoff and for the Brussels Effect. Both the US and China impose fewer obligations on open-source developers: the US has no federal open-source AI requirements, and China's registration system grants de facto forbearance to small and mid-sized enterprises. If the EU's provisions impose disproportionate compliance costs on European open-source developers, they risk widening the technology gap the Act partly aims to close. Regulatory sandboxes~\citep{madiega2024artificial} offer a partial safety valve, but the competitive implications warrant close monitoring by the AI Office.

\subsection{Implementation Progress (2025)}

Phased enforcement of the AI Act began on February 2, 2025, when the provisions banning prohibited AI practices, including social scoring, manipulative subliminal techniques, and most real-time remote biometric identification, became legally binding. On August 2, 2025, the GPAI model governance obligations took effect, requiring providers to comply with transparency and systemic-risk provisions, with penalties of up to \texteuro35~million or 7\% of global annual turnover. Full high-risk AI system requirements will apply from August 2026. With these milestones, the EU moved from legislative ambition to operational enforcement.

% ──────────────────────────────────────────────
\section{China}
% ──────────────────────────────────────────────

\subsection{The Hybrid Model: Centralised Guidance, Decentralised Enforcement}

China occupies the hybrid position in the governance typology: centralised strategic direction combined with decentralised, sector-specific implementation and selective enforcement. This combination challenges the binary framing that dominates much comparative governance literature, where jurisdictions are sorted into either ``top-down'' or ``bottom-up'' categories. In practice, China's central government sets ideological and strategic goals, while mid-level bureaucrats, academics, corporations, and local officials shape the regulatory details through an iterative stakeholder process~\citep{sheehan2024,zhang2022}. The result is a system that shares the EU's emphasis on safety and social stability, enforced through top-down guidance, while promoting bottom-up innovation through provincial competition and regulatory forbearance for smaller firms.

\subsection{Sector-Specific Regulation}

% Prevent the figure from moving out of this section
\FloatBarrier

Rather than the EU's horizontal risk classification, China has built AI governance through sector-specific regulations (Table~\ref{tab:china_laws}), covering data privacy (PIPL, 2021), recommendation algorithms (2022), deepfakes (January 2023), and generative AI (August 2023). This approach resembles the US model of domain-specific rules but differs in one key respect: all regulations are administered by the Cyberspace Administration of China (CAC), which also oversees internet content control. AI governance and internet governance are therefore institutionally unified under a single authority, a feature with no equivalent in either the EU or the US~\citep{ding2024chinaai}. Both the State Council and the Chinese Academy of Social Sciences have announced intentions to develop a unified National AI law, though the outcome remains uncertain~\citep{webster2023}.

\begin{table}[ht]
\centering\small
\begin{tabularx}{\linewidth}{@{}l >{\raggedright\arraybackslash}p{3.4cm} >{\raggedright\arraybackslash}p{2.6cm} X@{}}
\toprule
\textbf{Date} & \textbf{Title} & \textbf{Issuing Body} & \textbf{Description} \\
\midrule
June 1, 2017 & Cybersecurity Law & National People's Congress & Establishes legal frameworks for cybersecurity, including data protection and network security, which indirectly impact AI development and deployment. \\
\addlinespace
Sept.\ 1, 2021 & Data Security Law & National People's Congress & Provides regulations on data processing and security, affecting AI systems that process large amounts of data. \\
\addlinespace
Nov.\ 1, 2021 & Personal Information Protection Law (PIPL) & National People's Congress & China's comprehensive data privacy law governing the collection, storage, use, and transfer of personal information, impacting AI systems that handle personal data. \\
\addlinespace
March 1, 2022 & Algorithm Recommendation Regulation & Cyberspace Administration of China (CAC) & Regulates algorithms used for content recommendations, requiring transparency and fairness, and prohibiting practices that disrupt public order. \\
\addlinespace
Jan.\ 10, 2023 & Deep Synthesis Regulation & CAC & Governs generative AI technologies, focusing on the authenticity and traceability of AI-generated content to prevent misinformation. \\
\addlinespace
Aug.\ 15, 2023 & Generative AI Measures & CAC and six other authorities & Targets generative AI services, imposing obligations on service providers to ensure legality, fairness, and cybersecurity of AI-generated content. \\
\addlinespace
Oct.--Nov.\ 2022 & AI Industry Promotion Regulations (Shanghai \& Shenzhen) & Shanghai and Shenzhen Municipal Governments & Local regulations to promote AI development, including ethical oversight and support for innovation within the AI industry. \\
\addlinespace
Expected 2024/2025 & Draft Artificial Intelligence Law & State Council (drafting stage) & A comprehensive national AI law is being drafted, aiming to provide an overarching legal framework for AI governance in China. \\
\bottomrule
\end{tabularx}
\caption{Chinese AI laws and regulations.}
\label{tab:china_laws}
\end{table}

% Prevent the figure from moving out of this section
\FloatBarrier

\subsection{The Enforcement Gap: De Jure Strictness, De Facto Permissiveness}

% Prevent the figure from moving out of this section
\FloatBarrier

On paper, China's AI compliance requirements are the most onerous of the three jurisdictions (Table~\ref{tab:china_compliance}): model registration, training-data documentation, content censorship verification, algorithm registration, and ongoing monitoring. As of March 2024, only 546 AI models had been registered, with just seventy being LLMs~\citep{chinamoney2024}. That figure is dwarfed by the hundreds of thousands of models available on platforms like Hugging Face, which is blocked in China~\citep{huggingface2024,chinatalk2023}.

\begin{table}[ht]
\centering\small
\begin{tabularx}{\linewidth}{@{}>{\raggedright\arraybackslash}p{3.8cm} X@{}}
\toprule
\multicolumn{2}{@{}l}{\textbf{Model Registration}} \\
\midrule
Approval Process & AI models, especially generative and LLM, must undergo thorough review for compliance with regulatory standards by the CAC and other bodies. \\
Sector-Specific Approvals & Approval by additional sector-specific regulatory bodies may be required (e.g.\ healthcare, finance, security). \\
\midrule
\multicolumn{2}{@{}l}{\textbf{Data Management}} \\
\midrule
Privacy and Content & All training data must comply with PIPL, exclude politically sensitive content, and include foreign-language material for global competitiveness. \\
\midrule
\multicolumn{2}{@{}l}{\textbf{Compliance Obligations}} \\
\midrule
Security Assessment & Mandatory pre-launch security assessments and algorithm registration. \\
Ongoing Monitoring & Continuous monitoring for system errors or deviations from approved conditions. \\
\bottomrule
\end{tabularx}
\caption{AI model compliance steps in China.}
\label{tab:china_compliance}
\end{table}

What matters analytically is the gap between de jure requirements and de facto enforcement. ``National Champions'' like Baidu, Tencent, and Alibaba face full regulatory scrutiny, but the ``Made in China 2025'' industrial policy deliberately supports 10,000 ``Little Giants''---innovative SMEs that are informally afforded regulatory forbearance~\citep{mic2015,globaltimes2021,zhang2024}. Startups operate under the radar as long as they lack a large public presence~\citep{zhang2022,yang2024}. This selective enforcement is not lawlessness but a strategic choice: it allows China to maintain formal regulatory coherence while preserving the competitive dynamism that strict enforcement would suppress.

When destabilising patterns emerge, however, enforcement can turn punitive. Between 2020 and 2022, regulators imposed harsh penalties on tech giants including Alibaba and Ant Group to check concentrations of private power that could challenge government authority~\citep{chen2023}. Similar crackdowns occurred in real estate and private education~\citep{bloomberg2021,intresse2024}. This reactive enforcement pattern---permissive by default, punitive when threatened---amounts to a distinct resolution of the innovation--safety tradeoff, one that sacrifices regulatory predictability for strategic flexibility. For EU policymakers, the Chinese model offers a cautionary lesson: registration requirements alone do not determine regulatory burden; enforcement culture matters at least as much.

\subsection{Recent Developments (2025)}

China's regulatory apparatus continued to evolve in 2025. In August, the State Council issued the ``AI Plus'' Action Plan~\citep{china_aiplus_2025}, targeting 70\% AI penetration across key economic sectors by 2027 and 90\% adoption of next-generation AI agents by 2030. In October, the National People's Congress amended the Cybersecurity Law~\citep{china_cybersecurity_2025} to incorporate AI governance provisions for the first time in national legislation, including mandated AI ethics review and risk assessment, along with increased administrative penalties (from RMB~1 to 10~million). Mandatory AI content-labelling rules also took effect in September 2025. The pattern is consistent: centralised strategic direction at the top, pragmatic sector-specific implementation below.

For the Brussels Effect thesis, China's hybrid model has two implications. First, China's development of a substantive AI governance framework means the EU no longer has the field to itself: developing nations now have an alternative model to adopt. Second, China's emphasis on infrastructure diplomacy and technical standard-setting through bodies like the ITU creates a competing channel for regulatory influence that bypasses the legislative export mechanisms on which the Brussels Effect depends. European policymakers should not assume that the AI Act's extraterritorial reach will operate in a regulatory vacuum.

% ──────────────────────────────────────────────
\section{United States}
% ──────────────────────────────────────────────

\subsection{Distributed, Ex Post Governance}

In the governance typology, the United States occupies the distributed, ex post quadrant. Its regulatory tradition distributes oversight across specialised federal agencies (FTC, CPSC, CFPB, and others), rooted in an institutional distrust of centralised power reinforced by industry self-regulation and a \$46~billion lobbying ecosystem~\citep{massoglia2024,cusumano2021}. Technology companies routinely pursue self-regulation on privacy, content moderation, and cybersecurity~\citep{minow2023}, and the Biden administration initially followed this pattern by securing voluntary AI safety commitments from leading companies~\citep{whitehouse2023a}. Where the EU created a new centralised body and China unified AI governance under the CAC, the US has no single AI regulator and no integrated link between AI governance and internet platform regulation.

This distributed structure has both advantages and costs. The breadth of the US federal agency network means that domain expertise already exists across the sectors AI is disrupting: the FDA for healthcare AI, the SEC for financial algorithms, NIST for safety standards, and the FTC for consumer protection. The speed of AI innovation, combined with limited technical expertise in Congress, has shifted initiative to the executive branch and to state legislatures. Meanwhile, dozens of federal bills have been introduced, including more restrictive proposals such as Senator Schumer's SAFE initiative~\citep{schumer2023}, but as of late 2025 none have been enacted. As a result, guidelines, executive actions, and trade policies are scattered across branches, agencies, and states~\citep{perkins2024}.

\subsection{Executive Order 14110 and Its Revocation}

% Prevent the figure from moving out of this section
\FloatBarrier

EO~\#14110, signed in October 2023, was the most ambitious attempt to impose coherence on this distributed system~\citep{whitehouse2023b}. It assigned over one hundred specific tasks across eight policy areas (Table~\ref{tab:us_eo14110}) to more than fifty federal agencies, with 180- to 270-day deadlines, all of which were met~\citep{whitehouse2024b,whitehouse2024d}. The order addressed many of the same concerns as the EU AI Act (safety, bias, consumer protection, privacy) but through a fundamentally different mechanism: augmenting existing agencies' domain expertise rather than creating a centralised regulatory framework~\citep{crs2024}.

\begin{table}[ht]
\centering\small
\begin{tabularx}{\linewidth}{@{}c >{\raggedright\arraybackslash}p{3.6cm} >{\raggedright\arraybackslash}p{2.8cm} X@{}}
\toprule
\textbf{Relevance} & \textbf{Policy Area} & \textbf{Requirements / Entities} & \textbf{Federal Entities *} \\
\midrule
\multirow{3}{*}{\textbf{High}}
  & Federal use of AI        & 29 requirements, 40 entities & OMB, OPM, CFO, GSA, etc. \\
  & Safety and security      & 27 requirements, 30 entities & NIST, DOE, DOC, SRMA, Treasury, DHS, DOD \\
  & Innovation and competition & 21 requirements, 10 entities & DOS, DHS, DOL, NSF, USPTO, HHS, VA, DOE, PCAST, OSTP+ \\
\midrule
\multirow{4}{*}{\textbf{Medium}}
  & AI bias and civil rights & 9 requirements, 8 entities  & DOJ, OPM, HHS, USDA, DOL, HUD, DHS, OSTP \\
  & Consumer protection      & 9 requirements, 5 entities  & HHS, DOT, ED, DOD, VA \\
  & Privacy                  & 6 requirements, 9 entities  & OMB, NIST, NSF, FPC, ICSP, DOJ, CEA, OSTP, DOE \\
  & International leadership & 6 requirements, 7 entities  & DOC, DOS, USAID, DHS, NIST, DOE, NSF \\
\midrule
\textbf{Low}
  & Worker support           & 4 requirements, 2 entities  & CEA, DOL \\
\bottomrule
\end{tabularx}
\caption{Executive Order \#14110 on the Safe, Secure, and Trustworthy Development and Use of AI (* agency acronyms expanded in the text).}
\label{tab:us_eo14110}
\end{table}

Enforcement reveals the contrast with the EU. The EU's model is preventive: it prohibits activities unless explicitly permitted through conformity assessment. The US model is permissive: it promotes innovation through competition and relies on an extensive network of existing consumer-protection, IP, and tort law to address harms after they occur~\citep{walters2022}. The NIST AI Safety Institute, established in January 2024, exemplifies this approach: its task forces on safety, evaluation, and risk management draw on academia and industry through voluntary partnership rather than regulatory mandate~\citep{nist2024a,nist2024b}.

How fragile this approach can be became apparent on January 20, 2025, when President Trump revoked EO~\#14110 on his first day in office. His replacement, Executive Order~14179~\citep{trump2025eo14179}, mandates an AI Action Plan within 180 days but contains no specific safety requirements or compliance deadlines. The contrast exposes a structural difference between the EU and US approaches: the AI Act, as legislation, cannot be unilaterally rescinded; an executive order can. For firms making long-term compliance investments, this disparity in regulatory durability is itself a governance variable.

\subsection{State-Level AI Regulation: SB 1047, SB 53, and the RAISE Act}

% Prevent the figure from moving out of this section
\FloatBarrier

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/fig_casb1047_timeline.pdf}
    \caption{Action Timeline for SB 1047}
    \label{fig:fig_sb1047_timeline}
\end{figure}

With federal regulation in flux, state-level legislation has emerged as the primary site of US AI governance innovation. California's SB~1047~\citep{LegiScan2024}, introduced in February 2024, proposed the most EU-like approach within the US system: ex ante safety requirements for ``frontier models'' defined by compute thresholds, mandatory third-party audits, and incident reporting (Figure~\ref{fig:fig_sb1047_timeline}). The bill drew support from AI researchers and unions but vigorous opposition from industry groups and open-source advocates~\citep{thehill_ai_supporters_2024,nunez2024}.

Governor Newsom's September 2024 veto~\citep{newsom2024veto} turned on a distinction central to the governance typology: Newsom rejected compute-based thresholds (measuring \textit{technical potential}) in favour of deployment-context assessment (measuring \textit{actual risk}). This is the same distinction that separates the EU's ex ante risk classification from the US tradition of ex post, harm-based enforcement. The veto confirmed the gravitational pull of the distributed, ex post model even when individual actors within the US system attempt to move toward centralised, ex ante regulation.

What followed is revealing. California's SB~53~\citep{casb53_2025}, enacted in September 2025, replaced SB~1047's prescriptive safety mandates with transparency obligations: annual risk-governance frameworks, pre-deployment transparency reports, and 15-day incident reporting for frontier models exceeding $10^{26}$ FLOPs. New York's RAISE Act~\citep{ny_raise_2025} followed in December 2025 with annual independent audits and 72-hour incident reporting. These laws represent a pragmatic middle ground, transparency without ex ante prohibition, that is distinctly American: regulate through disclosure rather than through centralised pre-market assessment. For European policymakers, this trajectory suggests that US AI governance will not converge toward the EU model but will instead develop its own transparency-centred alternative.

% ──────────────────────────────────────────────
\section{Discussion and Conclusion}
% ──────────────────────────────────────────────

\subsection{Comparative Synthesis}

Applying the governance typology from Section~2, the three cases confirm the predicted positions but also reveal dynamics that a static classification would miss. The EU sits squarely in the centralised, ex ante quadrant: the AI Act establishes an integrated, risk-based framework that regulates AI systems \textit{before} they reach the market. At the opposite corner, the US distributes responsibilities across existing agencies and relies on tort, consumer-protection, and IP law for enforcement. China falls between the two, combining centralised guidance and formal registration requirements with decentralised, sector-specific enforcement and deliberate forbearance for innovative SMEs.

These positions are not static, but institutional traditions exert a gravitational pull. The SB~1047 veto and the Trump administration's revocation of EO~\#14110~\citep{trump2025eo14179} both illustrate this force: when individual actors within the US system attempted to move toward centralised, ex ante regulation, the institutional default reasserted itself. State-level successors (SB~53, the RAISE Act) settled on transparency-based obligations rather than ex ante prohibition, a distinctly American resolution. China's hybrid model shows a different dynamic: formal rules tighten over time (the 2025 Cybersecurity Law amendments doubled maximum penalties), but enforcement remains strategically selective, suggesting the hybrid position is stable rather than transitional.

On the Brussels Effect, the evidence offers only partial support. The AI Act's extraterritorial reach creates compliance incentives for global firms. Yet three conditions that enabled the GDPR's influence (first-mover advantage, absence of competing models, and low compliance costs) do not hold for AI governance. The simultaneous emergence of substantive frameworks in China and the US limits unilateral standard-setting. Rather than a single Brussels Effect, AI governance is evolving toward a \textit{multipolar regulatory landscape} in which firms manage competing compliance regimes \citep{bradford_brussels_2020}. This finding aligns with recent IPR scholarship questioning the reach of European regulatory influence in domains where non-European alternatives exist~\citep{pohle2020digital}.

\subsection{Implications for Internet Governance}

Divergent AI governance has direct consequences for internet regulation, the core concern of this journal's readership. Each jurisdiction embeds AI governance within a different internet-governance architecture, producing distinct outcomes for platform accountability and algorithmic transparency~\citep{helberger2020}.

Of the three, the EU has constructed the most integrated framework. The AI Act operates alongside the DSA and DMA, meaning that AI-powered content moderation, recommendation algorithms, and platform decision-making are subject to overlapping requirements: DSA transparency obligations for algorithmic recommender systems \textit{and} AI Act risk-based requirements if those systems qualify as high-risk. This layered approach gives the EU the broadest toolkit for governing AI's role in online information ecosystems, but it also creates compliance complexity that may disadvantage smaller platforms and open-source providers relative to large incumbents capable of absorbing regulatory overhead.

China's unified approach under the CAC treats AI governance and internet content control as a single policy domain. The Algorithm Recommendation Regulation (2022) and the Generative AI Measures (2023) are administered by the same authority that oversees internet censorship, making AI regulation an extension of content governance rather than a separate policy area. This institutional unification produces coherence at the cost of conflating content moderation with political control.

No integrated framework links US AI governance to platform regulation. Without a federal equivalent to the DSA, AI-driven content decisions (recommendation algorithms, automated moderation, deepfake generation) remain largely self-regulated. Section~230 of the Communications Decency Act continues to shield platforms from liability for user-generated content, and no federal AI law addresses the algorithmic amplification of that content. For European policymakers, this gap represents both a risk (US-based platforms operating in the EU face a compliance patchwork) and an opportunity (the EU's integrated approach may attract developing nations seeking a coherent model for governing AI-mediated online spaces).

\subsection{Implications for European Policy}

Several concrete implications follow as the AI Act enters its implementation phase.

First, the EU's \textit{adaptive legislation} approach, deliberately leaving technical details open, is both a strength and a vulnerability. The Chinese experience with sector-specific regulations demonstrates that targeted, use-case-driven rules can be implemented faster and with less ambiguity. The AI Office should prioritise rapid issuance of domain-specific compliance guidance for each Annex~III use case within twelve months of the high-risk provisions taking effect in August 2026, drawing on the sector expertise of national market-surveillance authorities rather than developing all guidance centrally.

Second, the open-source tension requires early attention. The EU Act's partial exemptions are narrower than the de facto permissive environment in both the US and China. The AI Office should issue interpretive guidance clarifying how GPAI transparency obligations apply to community-developed models and establish a fast-track compliance pathway for open-source providers below a defined revenue threshold.

Third, the US experience exposes a structural advantage of legislation over executive action. EO~\#14110 achieved over one hundred compliance milestones within 270 days but was revoked with a single presidential signature. The AI Act's legislative durability provides the regulatory stability that firms need for long-term compliance investment. To capitalise on this advantage, the AI Office should build strategic partnerships with member-state regulators, giving national data-protection authorities and market-surveillance bodies front-line enforcement responsibilities through formal delegation agreements.

Fourth, growing geopolitical competition (tariffs, chip export controls, sanctions on strategic technologies) means AI regulation cannot be analysed in isolation from trade and industrial policy. Innovation-focused deregulation in the US may erode the Brussels Effect, but it also opens space for European standards to serve as a ``third way'' between US permissiveness and Chinese state control. European policymakers should pursue mutual recognition agreements with jurisdictions whose AI governance frameworks are converging with the EU model, beginning with countries that have already adopted GDPR-influenced data-protection regimes~\citep{goddard2017eu}.

% ──────────────────────────────────────────────
\section*{Author Contributions}
\addcontentsline{toc}{section}{Author Contributions}
% ──────────────────────────────────────────────
Jon Chun led the analysis of the United States and China sections. Christian Schroeder de Witt led the analysis of the European Union section. Katherine Elkins contributed to the United States section. All authors contributed to the theoretical framework, methodology, and comparative synthesis. Each author has reviewed the complete manuscript.

% ──────────────────────────────────────────────
% Author biographies (IPR requirement)
% ──────────────────────────────────────────────
\section*{About the Authors}
\addcontentsline{toc}{section}{About the Authors}

\textbf{Jon Chun} is Visiting Instructor of Humanities at Kenyon College. He holds graduate degrees in computer science and electrical engineering from UC Berkeley and UT Austin, with extensive industry experience in FinTech and cybersecurity. He is lead investigator for the Modern Language Association's participation in the NIST US AI Safety Institute and co-principal investigator on an IBM--Notre Dame Tech Ethics Lab grant on AI decision-making in criminal justice. His research spans human-centred AI, AI safety, and technology policy.

\textbf{Christian Schroeder de Witt} is Lecturer in the Department of Engineering Science at the University of Oxford and Principal Investigator of the Oxford Witt Lab for Trust in AI. A Royal Academy of Engineering Research Fellow and Schmidt Sciences AI2050 Fellow, his work addresses multi-agent security and AI assurance. He advises RAND, the BBC, and the UK government on AI governance and risk.

\textbf{Katherine Elkins} is Professor of Comparative Literature and Humanities at Kenyon College, with affiliated faculty status in Computing. She co-founded Kenyon's human-centred AI curriculum in 2016 and is co-principal investigator for both the NIST US AI Safety Institute and the IBM--Notre Dame Tech Ethics Lab grant on AI decision-making in recidivism cases. Her research connects computational methods with humanistic inquiry to inform policy on AI safety, bias, and governance.

% Prevents floating elements from causing a blank page
\FloatBarrier

% ──────────────────────────────────────────────
% References
% ──────────────────────────────────────────────
\printbibliography

% Insert a new page before Appendix A
\clearpage

% Start Appendix A with the correct title
\section*{Appendix A: Comparative Regulatory Provisions}
\addcontentsline{toc}{section}{Appendix A: Comparative Regulatory Provisions}

\small
\begin{longtable}{@{}>{\raggedright\arraybackslash}p{2.8cm} >{\raggedright\arraybackslash}p{3.8cm} >{\raggedright\arraybackslash}p{3.8cm} >{\raggedright\arraybackslash}p{3.8cm}@{}}
\toprule
\textbf{Dimension} & \textbf{EU} & \textbf{China} & \textbf{US} \\
\midrule
\endfirsthead
\toprule
\textbf{Dimension} & \textbf{EU} & \textbf{China} & \textbf{US} \\
\midrule
\endhead
\textbf{Primary instrument} & AI Act (Regulation 2024/123) & Suite of sector-specific regulations (2017--2025) & EO \#14110 (revoked Jan 2025); state laws (SB~53, RAISE Act) \\
\addlinespace
\textbf{Governance model} & Centralised, ex ante risk classification & Hybrid: centralised guidance, decentralised enforcement & Distributed, ex post enforcement via existing agencies \\
\addlinespace
\textbf{Scope} & All AI systems placed on or used in the EU market & Sector-specific: algorithms, deepfakes, generative AI, data & Federal: voluntary + agency mandates; states: frontier models only \\
\addlinespace
\textbf{Risk classification} & 4 tiers: prohibited, high, limited, minimal & No unified risk tiers; sector-by-sector requirements & No federal risk tiers; SB~53 applies to models $>10^{26}$ FLOPs \\
\addlinespace
\textbf{Enforcement body} & EU AI Office + member-state authorities & CAC + sector regulators + local governments & No central body; FTC, NIST, CISA, state AGs \\
\addlinespace
\textbf{Open-source treatment} & Partial exemptions (not for GPAI of systemic risk) & No formal exemptions; de facto permissive for SMEs & No federal provisions; generally permissive \\
\addlinespace
\textbf{Extraterritorial reach} & Yes (output used in EU) & Limited (data localisation requirements) & No (state laws apply to in-state operations) \\
\addlinespace
\textbf{Penalties} & Up to \texteuro35M or 7\% global turnover & Up to RMB~10M (post-2025 amendments) & No federal penalties; state civil penalties vary \\
\addlinespace
\textbf{Transparency} & GPAI training data summaries; high-risk documentation & Algorithm registration; AI content labelling & SB~53: annual risk reports, 15-day incident reporting \\
\addlinespace
\textbf{Internet governance link} & Integrated with DSA, DMA, and NIS2 & Unified under CAC (internet + AI + data) & No integrated framework \\
\bottomrule
\caption{Comparative overview of AI regulatory provisions across the EU, China, and the US.}
\end{longtable}

\end{document}
