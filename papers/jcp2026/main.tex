\documentclass[a4paper,12pt]{article}

% ──────────────────────────────────────────────
% Font — Times New Roman (JCP / Taylor & Francis)
% ──────────────────────────────────────────────
\usepackage[T1]{fontenc}
\usepackage{mathptmx}   % Times-compatible body text and maths

% ──────────────────────────────────────────────
% Page layout — A4, 2.5 cm margins, double spacing
% ──────────────────────────────────────────────
\usepackage[a4paper, margin=2.5cm]{geometry}
\usepackage[doublespacing]{setspace}
\usepackage[parfill]{parskip}

% ──────────────────────────────────────────────
% Section headings — bold, serif (Times)
% ──────────────────────────────────────────────
\usepackage{titlesec}
\titleformat{\section}{\Large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\large\bfseries}{\thesubsection}{1em}{}
\titleformat{\subsubsection}{\normalsize\bfseries}{\thesubsubsection}{1em}{}

% ──────────────────────────────────────────────
% Notes — Chatham House numbered endnotes
% ──────────────────────────────────────────────
\usepackage{endnotes}
\let\footnote=\endnote
\renewcommand{\notesname}{Notes}

% ──────────────────────────────────────────────
% Bibliography — biblatex verbose-note for footnote-style citations
% ──────────────────────────────────────────────
\usepackage[hyphens]{url}
\usepackage{csquotes}
\usepackage[
  style=verbose-note,
  backend=biber,
  abbreviate=true,
  ibidtracker=false
]{biblatex}
\addbibresource{main.bib}

% Remap non-standard entry types to @misc
\DeclareSourcemap{
  \maps[datatype=bibtex]{
    \map{
      \step[typesource=legal, typetarget=misc]
    }
    \map{
      \step[typesource=legislation, typetarget=misc]
    }
    \map{
      \step[typesource=report, typetarget=misc]
    }
  }
}

% ──────────────────────────────────────────────
% Figures and tables
% ──────────────────────────────────────────────
\usepackage{graphicx}
\usepackage{float}
\usepackage{placeins}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{longtable}
\usepackage{array}
\usepackage{adjustbox}

% ──────────────────────────────────────────────
% Hyperlinks and URLs
% ──────────────────────────────────────────────
\usepackage{xcolor}
\usepackage[breaklinks,colorlinks,linkcolor=blue!70!black,citecolor=blue!70!black,urlcolor=blue!70!black]{hyperref}
\usepackage{bookmark}

% ──────────────────────────────────────────────
% Miscellaneous
% ──────────────────────────────────────────────
\usepackage{enumitem}

\DeclareUnicodeCharacter{2060}{}

% ──────────────────────────────────────────────
% Metadata
% ──────────────────────────────────────────────
\title{\textbf{The Cyber Governance Trilemma:\\Comparative AI Regulation in the EU, China,\\and the United States}}

\author{
    Jon Chun\textsuperscript{1} \\
    Christian Schroeder de Witt\textsuperscript{2} \\
    Katherine Elkins\textsuperscript{1} \\
    \\
    \textsuperscript{1}Kenyon College \\
    \textsuperscript{2}University of Oxford
}

\date{}  % JCP provides its own date in production

% ══════════════════════════════════════════════
\begin{document}
% ══════════════════════════════════════════════

\maketitle

% ──────────────────────────────────────────────
\begin{abstract}
Artificial intelligence regulation has emerged as a defining challenge for international cyber governance. As AI systems become deeply embedded in critical infrastructure, cybersecurity operations, and information ecosystems, the frameworks governing their development and deployment increasingly shape the global cyber policy landscape. This paper presents a comparative analysis of the three most influential AI regulatory approaches: the EU AI Act, China's sector-specific regulations, and the US approach spanning Executive Order~\#14110, its revocation under the Trump administration, and state-level initiatives including California's SB~53. Drawing on regulatory competition theory, a governance typology grounded in comparative regulatory capitalism, and the concept of regime complexity, we argue that these three jurisdictions face a \textit{cyber governance trilemma}: no jurisdiction can simultaneously optimise innovation speed, safety and rights protection, and regulatory interoperability. The EU prioritises safety at the cost of innovation velocity; the US prioritises innovation at the cost of consistent safety guarantees; China prioritises state-directed competitiveness at the cost of international interoperability. This regulatory fragmentation is itself a cyber risk, creating compliance gaps that sophisticated threat actors can exploit and complicating international norm-building processes. Incorporating developments through late 2025, we conclude with implications for international cyber governance, including lessons for developing nations navigating between competing regulatory models.
\end{abstract}

\medskip
\noindent\textbf{Keywords:} AI regulation, cyber governance, regulatory fragmentation, EU AI Act, comparative policy, Brussels Effect

% ──────────────────────────────────────────────
\section{Introduction}
% ──────────────────────────────────────────────

The regulation of artificial intelligence has become inseparable from cyber governance. AI-generated deepfakes now undermine election integrity and enable sophisticated social engineering attacks; autonomous agents probe critical infrastructure for vulnerabilities at machine speed; and adversarial actors exploit large language models to generate polymorphic malware that evades signature-based defences.\autocite{kello2017virtual} At the same time, AI systems have become indispensable to cyber defence, powering anomaly detection in financial networks, automating threat intelligence, and orchestrating incident response across complex digital ecosystems. The frameworks that govern AI development and deployment therefore shape the cyber threat landscape directly: how a jurisdiction regulates AI determines not only the safety of its own digital infrastructure but the security posture of every interconnected system.\autocite{nye2014regime}

As jurisdictions race to regulate these dual-use capabilities, the resulting patchwork of divergent frameworks creates a new form of systemic cyber risk: regulatory fragmentation. Where cybersecurity once centred on technical standards and incident response, the governance challenge has expanded to encompass the foundational question of how societies choose to develop, deploy, and constrain AI.\autocite{broeders2017public} The divergent answers emerging from the EU, China, and the United States carry direct implications for domestic technology policy and for the architecture of international cyber governance. Gaps between regulatory regimes create seams that sophisticated threat actors can exploit: an AI system that satisfies Chinese content-labelling requirements may violate EU transparency standards, while a system compliant with the EU AI Act's high-risk provisions may be entirely unregulated under the post-2025 US framework.

A growing body of scholarship has examined how different jurisdictions are approaching AI governance. Early comparative work mapped the ethical principles underpinning national AI strategies in the US, EU, and UK,\autocite{cath2018artificial}\autocite{floridi2018ai4people} while Stix identified three distinct policy pathways (ethics-based, rights-based, and risk-based) that governments have pursued.\autocite{stix2021actionable} Smuha documented the emergence of `regulatory competition' in AI, arguing that jurisdictions are now racing not only to develop AI but to set the rules governing it.\autocite{smuha2021race} Studies have examined individual regimes in depth, including the EU AI Act,\autocite{veale2021demystifying} China's layered regulatory approach,\autocite{roberts2021chinese} and the broader framing contests that shape AI policy.\autocite{ulnicane2021framing} However, few studies offer a systematic three-way comparison that situates the EU, China, and the US within a unified analytical framework or examines AI regulation through the lens of cyber governance: a gap this paper seeks to address.

The regulatory landscape has shifted dramatically since 2024. The EU AI Act's prohibitions on high-risk AI practices took effect in February 2025. In the United States, President Trump revoked Executive Order~\#14110 on his first day in office in January 2025, while California enacted a successor transparency law (SB~53) in September 2025 and New York passed the RAISE Act in December 2025. China amended its Cybersecurity Law in October 2025 to incorporate AI governance provisions for the first time in national legislation. AI regulation is evolving rapidly and unevenly across jurisdictions, with direct consequences for the coherence of international cyber governance.

The paper contributes in three ways. It provides a systematic comparative analysis of AI regulatory frameworks across the three major AI economies, updated through late 2025. It introduces a \textit{cyber governance trilemma}, the proposition that jurisdictions cannot simultaneously optimise innovation speed, safety and rights protection, and regulatory interoperability, and shows how each jurisdiction resolves this trilemma differently. And it draws out implications for international cyber governance, including for developing nations caught between competing regulatory models.

% ──────────────────────────────────────────────
\section{Theoretical Framework}
% ──────────────────────────────────────────────

Comparative studies of technology regulation have long observed that jurisdictions adopt divergent governance strategies shaped by institutional traditions, political economy, and geopolitical positioning.\autocite{cath2018artificial}\autocite{ulnicane2021framing} As Smuha argues, the global `race to AI regulation' now rivals the race to develop the technology itself.\autocite{smuha2021race} This paper draws on four complementary theoretical perspectives to move beyond description towards analytical comparison.

First, we draw on \textit{regulatory competition theory}, which holds that jurisdictions compete, and learn from one another, through their regulatory choices.\autocite{bradford_brussels_2020} Bradford's concept of the `Brussels Effect' describes a mechanism by which the EU's large single market incentivises global firms to adopt EU standards unilaterally. We examine whether the EU AI Act exhibits this dynamic and how competing regulatory models may limit it.

Second, we employ a \textit{governance typology} that distinguishes regulatory frameworks along two dimensions: (1)~the degree of \textit{centralisation} (top-down versus distributed) and (2)~the primary \textit{regulatory modality} (ex ante risk-based rules versus ex post enforcement through existing legal frameworks). This typology draws on Majone's analysis of the `regulatory state' in Europe\autocite{majone1994rise} and Levi-Faur's account of the `global diffusion of regulatory capitalism',\autocite{levifaur2005global} applying their comparative frameworks to the AI domain. The typology reveals that the three jurisdictions occupy distinct positions: the EU favours centralised, ex ante regulation; the US favours distributed, ex post enforcement; and China occupies a hybrid position combining centralised guidance with decentralised, sector-specific implementation.

Third, we attend to the \textit{innovation--safety tradeoff} that pervades AI governance debates globally.\autocite{stix2021actionable} Each jurisdiction resolves this tension differently, reflecting deeper differences in political values: the EU privileges precaution and fundamental rights,\autocite{veale2021demystifying} China prioritises social stability alongside state-directed industrial competitiveness,\autocite{roberts2021chinese} and the US emphasises market-driven innovation with voluntary commitments.

Fourth, we draw on Nye's concept of \textit{regime complexity}\autocite{nye2014regime} and the broader literature on internet governance fragmentation\autocite{mueller2017internet}\autocite{denardis2014global} to situate AI regulation within the evolving architecture of cyber governance. The absence of a single overarching regime for AI governance, mirroring the fragmented regime complex that characterises cyberspace more broadly, has direct implications for international cooperation, norm-building, and the management of transboundary cyber risks.\autocite{eichensehr2019cyber}

% ──────────────────────────────────────────────
\section{Methodology}
% ──────────────────────────────────────────────

This study employs a comparative case study methodology to examine how AI regulatory frameworks interact with, and reshape, the cyber governance landscape across three jurisdictions: the European Union, the People's Republic of China, and the United States (at both federal and state levels). Case selection follows a most-different-systems design: the three cases represent the world's largest AI economies while instantiating distinct governance traditions: centralised ex ante regulation, hybrid state-guided governance, and distributed market-oriented oversight. This design allows identification of structural factors that drive regulatory divergence and its cyber governance consequences.

Our analytical approach combines \textit{legal and policy document analysis} with \textit{regime complexity mapping}. For each jurisdiction, we examined the principal legislative texts, executive orders, regulatory guidance, and enforcement actions current through late 2025, with particular attention to provisions governing AI systems in cybersecurity-relevant domains: critical infrastructure, defence and intelligence, surveillance, and information integrity. We supplement primary legal sources with peer-reviewed scholarship, policy institute reports, and practitioner commentary. The comparison is structured around the governance typology and regime complexity frameworks introduced above, with the comparative regulatory timeline (Section~4) serving as an original empirical anchor for tracking the pace, sequencing, and competitive dynamics of regulatory activity across jurisdictions.

% ──────────────────────────────────────────────
\section{Comparative Regulatory Timeline}
% ──────────────────────────────────────────────

Table~\ref{tab:timeline} presents a comparative regulatory timeline constructed as a structured dataset of AI governance milestones across the three jurisdictions. Inclusion criteria were: (1)~binding legislation, executive orders, or regulations with direct AI governance provisions; (2)~institutional actions establishing new AI-specific governance bodies; and (3)~enforcement milestones marking the operational commencement of regulatory obligations. Non-binding strategies, voluntary commitments, and subnational actions below the state or provincial level were excluded unless they demonstrably shaped national regulatory trajectories (as California's SB~1047 debate did for US AI governance).

\begin{table}[ht]
\centering\small
\begin{tabularx}{\linewidth}{@{}l l X@{}}
\toprule
\textbf{Date} & \textbf{Jurisdiction} & \textbf{Milestone} \\
\midrule
Nov 2016 & China & Cybersecurity Law adopted (effective June 2017) \\
Apr 2018 & EU & AI Strategy: \textit{Artificial Intelligence for Europe} \\
Feb 2019 & US & Executive Order 13859 on maintaining US leadership in AI \\
Apr 2021 & EU & AI Act proposed by European Commission \\
Sept 2021 & China & Data Security Law takes effect \\
Nov 2021 & China & Personal Information Protection Law (PIPL) takes effect \\
Mar 2022 & China & Algorithm Recommendation Regulation takes effect \\
Oct 2022 & US & Blueprint for an AI Bill of Rights \\
Jan 2023 & China & Deep Synthesis Regulation takes effect \\
Aug 2023 & China & Generative AI Measures take effect \\
Oct 2023 & US & Biden signs EO~\#14110 (100+ delegated tasks to 50+ agencies) \\
Dec 2023 & EU & AI Act provisionally agreed (European Parliament + Council) \\
Jan 2024 & US & NIST establishes US AI Safety Institute \\
Aug 2024 & EU & AI Act enters into force \\
Sept 2024 & US & Governor Newsom vetoes California SB~1047 \\
Jan 2025 & US & Trump revokes EO~\#14110; signs EO~14179 (3 days later) \\
Feb 2025 & EU & Prohibited AI practices take effect \\
Aug 2025 & EU & GPAI model governance obligations take effect \\
Aug 2025 & China & State Council issues `AI Plus' Action Plan \\
Sept 2025 & US & California enacts SB~53 (Transparency in Frontier AI Act) \\
Oct 2025 & China & NPC amends Cybersecurity Law to incorporate AI governance \\
Dec 2025 & US & New York enacts RAISE Act \\
Aug 2026 & EU & High-risk AI system requirements take effect (scheduled) \\
\bottomrule
\end{tabularx}
\caption{Comparative timeline of major AI regulatory milestones (2016--2026).}
\label{tab:timeline}
\end{table}

Several patterns emerge from the timeline. First, the data reveal a clear acceleration: only three entries fall before 2021, while fifteen fall between 2023 and 2026, reflecting the post-ChatGPT urgency that swept all three jurisdictions simultaneously. Second, the jurisdictions exhibit distinct sequencing strategies. China led with targeted, sector-specific regulations (algorithmic recommendation, deepfakes, generative AI) before attempting consolidation through Cybersecurity Law amendments in 2025. The EU pursued a single comprehensive legislative instrument over a multi-year process, accepting slower adoption in exchange for systemic coherence. The US oscillated between executive action and state-level experimentation, with the revocation of EO~\#14110 representing a dramatic reversal in federal-level ambition. Third, the timeline reveals periods of competitive acceleration, notably the 2023--2025 cluster in which all three jurisdictions undertook major regulatory actions within months of each other, suggesting the regulatory competition dynamics that Smuha theorised.\autocite{smuha2021race} For cyber governance, the acceleration pattern is consequential: rapid, uncoordinated regulatory proliferation across jurisdictions compounds the interoperability challenges that the trilemma describes.

% ──────────────────────────────────────────────
\section{The European Union}
% ──────────────────────────────────────────────

\subsection{Overview}

The 2024 EU AI Act is positioned as the world's first comprehensive AI law.\autocite{eu_ai_act_2024} Just as prior European general-purpose legislation, such as the 2016 General Data Protection Regulation (GDPR),\autocite{gdpr_article7} the EU AI Act represents complex joint efforts across various EU bodies, including the European Commission, the European Parliament, and the European Council. Influence on the Act's formation was taken publicly by national government officials, such as France's Macron lobbying for exemptions for open-source AI providers such as Mistral,\autocite{abboud_eus_2023} as well as by lobbying and industry groups, including Big Tech\autocite{perrigo_exclusive_2023} and German pro-open-source non-profit LAION.\autocite{noauthor_call_nodate}

The Act was first constructed within a product safety framework, but then blended with a fundamental rights agenda at the behest of the European Parliament.\autocite{caroli_podcast} This approach resulted in a unique syncretism, clearly setting the EU AI Act apart from prior legislation building on established frameworks such as the GDPR. The Act also follows earlier European regulatory initiatives, including the 2022 Digital Markets Act\autocite{dma} and, of direct relevance to cyber governance, the NIS2 Directive on cybersecurity.\autocite{eu_nis2_2022} The interaction between the AI Act and NIS2 creates the most integrated AI-cybersecurity governance framework among the three jurisdictions examined here. The NIS2 Directive, which entered into force in January 2023 and required member-state transposition by October 2024, designates operators in energy, transport, banking, health, digital infrastructure, and public administration as `essential entities' subject to mandatory cybersecurity risk management and incident reporting. Where AI systems are deployed within these sectors (as they increasingly are, for grid management, fraud detection, medical diagnostics, and network traffic analysis) providers must satisfy both the AI Act's risk-based requirements and NIS2's cybersecurity obligations simultaneously. This regulatory overlap is deliberate: it reflects the EU's systemic approach to digital governance, in which AI safety, data protection (GDPR), and cybersecurity (NIS2) form an interlocking regulatory architecture.

\subsection{The Geopolitics of the Act}

Besides regulating the EU single market, the Act is widely regarded as a strategic effort by the European Commission to establish themselves as the leading AI rulemakers globally.\autocite{noauthor_eus_2024} It is speculated that companies across the world will begin to prioritise compliance with European AI law out of economic necessity, not through coercion: the `Brussels Effect'.\autocite{bradford_brussels_2020}\autocite{almada_brussels_2024} One direct institutional consequence is the establishment of the EU AI Office, which will oversee AI regulation, provide a central pool of AI expertise to member states, and support a `strategic, coherent, and effective European approach on AI at the international level'.\autocite{eu_ai_office}

\subsection{Risk Classification and Regulation}

The Act is designed as an \textit{adaptive legislation}, with details intentionally left open for later specification. At its core lies a risk classification system that places obligations primarily on the developers (`providers') of AI systems. The Act applies not only to systems placed on the market within the EU, but also to AI systems whose output is used in the EU, giving the Act considerable extraterritorial reach.

The Act defines four risk tiers. \textit{Prohibited practices} include AI systems used for social scoring, manipulative subliminal techniques, and most real-time remote biometric identification.\autocite[Article 5]{eu_ai_act_2024} \textit{High-risk systems} constitute the majority of regulation, requiring risk management systems, data governance, human oversight, and cybersecurity measures.\autocite[Article 6]{eu_ai_act_2024} \textit{Limited-risk} systems face transparency obligations, while \textit{minimal-risk} systems are left unregulated.

For cyber governance, the high-risk category matters most. AI systems used as safety components of critical infrastructure (Annex~III, Area~2), AI systems used for law enforcement and border control (Annex~III, Areas~6--7), and AI systems deployed in democratic processes all fall within this tier. Article~15 of the Act explicitly requires providers of high-risk AI systems to achieve an `appropriate level of cybersecurity' proportionate to the risks, including resilience against adversarial manipulation, data poisoning, and model extraction attacks, requirements that mirror and reinforce NIS2 obligations for essential entities. This is the clearest example in any jurisdiction of AI regulation being explicitly linked to cybersecurity standards.

Among models, the Act further distinguishes \textit{general-purpose AI (GPAI) models}, with heightened obligations for those posing `systemic risk', defined as models trained with cumulative compute exceeding $10^{25}$ floating-point operations.\autocite{eu_ai_act_2024} Providers must register with the European Commission and adhere to wide-ranging safety and security criteria, including adversarial testing and model evaluation.

\subsection{Open Source and Innovation}

The Act contains wide-ranging exemptions for providers of AI systems under free and open-source software licences,\autocite[Article 53--54]{eu_ai_act_2024} provided they do not contain GPAI models of systemic risk. However, the partial nature of these exemptions, narrower than the de facto permissive environment in both the US and China, remains a critical unresolved tension.

\subsection{Implementation Progress (2025)}

By late 2025, the AI Act had moved from legislative ambition to operational enforcement. The prohibited-practices provisions, banning social scoring, manipulative subliminal techniques, and most real-time remote biometric identification, became legally binding on 2~February 2025. GPAI model governance obligations followed on 2~August 2025, with penalties reaching \texteuro35~million or 7\% of global annual turnover for non-compliance. Full high-risk AI system requirements are scheduled for August 2026. In parallel, NIS2 transposition deadlines drove member states to integrate AI-specific cybersecurity obligations into national frameworks, creating a synchronised rollout of AI safety and cybersecurity governance unprecedented among the three jurisdictions.

% ──────────────────────────────────────────────
\section{China}
% ──────────────────────────────────────────────

\subsection{Overview}

China's approach to AI governance occupies a distinctive hybrid position: centralised strategic direction combined with decentralised, sector-specific implementation and selective enforcement. The central government sets overarching objectives (social stability, technological self-sufficiency, and `core socialist values') while provincial governments and sectoral regulators translate these into operational rules, often with considerable latitude for local experimentation.\autocite{zhang2022} The design is a deliberate attempt to take the best of both the EU model (coherent top-down guidance) and the US model (market-driven innovation) while avoiding the perceived weaknesses of each.

What sets China apart from a cyber governance perspective is its institutional architecture. The Cyberspace Administration of China (CAC) is simultaneously the country's internet regulator, data protection authority, and primary AI governance body. This consolidation is deliberate: it ensures that AI regulation operates within, and reinforces, China's broader cyber sovereignty strategy, in which data governance, content control, algorithmic regulation, and cybersecurity are a single governance ecosystem rather than separate policy domains.

\subsection{Laws and Regulations}

\FloatBarrier

China has advanced some of the first AI laws and regulations at the national level, summarised in Table~\ref{tab:china_laws}. Unlike the horizontal risk-based approach of the EU, China has favoured the sector-specific US approach of laws tailored to specific use cases. Despite appearances of centralised government control, Chinese AI regulations are the product of an iterative process involving diverse stakeholders including mid-level bureaucrats, academics, corporations, startups, and think tanks.\autocite{sheehan2024} The central government relies upon a pipeline of experts to formulate and interpret the details, while local officials mainly ensure alignment with Chinese and socialist ideology.\autocite{zhang2022}

\begin{table}[ht]
\centering\small
\begin{tabularx}{\linewidth}{@{}l >{\raggedright\arraybackslash}p{3.4cm} >{\raggedright\arraybackslash}p{2.6cm} X@{}}
\toprule
\textbf{Date} & \textbf{Title} & \textbf{Issuing Body} & \textbf{Description} \\
\midrule
June 2017 & Cybersecurity Law & National People's Congress & Legal frameworks for cybersecurity, data protection, and network security, indirectly impacting AI. \\
\addlinespace
Sept.\ 2021 & Data Security Law & National People's Congress & Regulations on data processing and security affecting AI systems. \\
\addlinespace
Nov.\ 2021 & Personal Information Protection Law (PIPL) & National People's Congress & Comprehensive data privacy law governing collection, storage, and transfer of personal information. \\
\addlinespace
Mar.\ 2022 & Algorithm Recommendation Regulation & CAC & Regulates recommendation algorithms, requiring transparency and fairness. \\
\addlinespace
Jan.\ 2023 & Deep Synthesis Regulation & CAC & Governs generative AI, focusing on authenticity and traceability of AI-generated content. \\
\addlinespace
Aug.\ 2023 & Generative AI Measures & CAC and six other authorities & Obligations on generative AI service providers for legality, fairness, and cybersecurity. \\
\bottomrule
\end{tabularx}
\caption{Chinese AI laws and regulations.}
\label{tab:china_laws}
\end{table}

\FloatBarrier

\subsection{Registration, Compliance, and Industrial Policy}

On paper, China has perhaps the most onerous AI regulation requirements of the three regions considered. Model registration, data management rules, and ongoing compliance monitoring illustrate how strict central regulation can slow innovation. As of March 2024, only 546 AI models had been registered, and just seventy were Large Language Models\autocite{chinamoney2024}, in stark contrast to the over 500,000 open-source LLMs on Huggingface.co,\autocite{huggingface2024} which is banned in China.\autocite{chinatalk2023}

Yet in practice, enforcement is deliberately selective. China's `Made In China 2025' industrial policy supports 10,000 `Little Giants', small and mid-sized enterprises recognised as key sources of innovation,\autocite{globaltimes2021} which are informally afforded regulatory leeway.\autocite{zhang2024} Startups and SMEs largely operate beneath the enforcement threshold as long as they lack a large public presence.\autocite{zhang2022} Selective enforcement is also accompanied by a growing network of municipal AI regulatory sandboxes. Shanghai and Shenzhen have both established AI pilot zones that allow companies to test novel AI applications under relaxed compliance requirements, mirroring the EU AI Act's sandbox provisions but with characteristically less formal structure. The Shanghai AI pilot zone, for instance, permits experimentation with autonomous driving, medical AI diagnostics, and smart-city applications under streamlined registration procedures, providing a controlled pathway for innovation that the national regulatory framework does not yet accommodate.

China's selective approach also has an international dimension. Through the Belt and Road Initiative's digital infrastructure investments, Chinese AI companies, and the regulatory norms embedded in their systems, are being exported to Southeast Asia, Central Asia, and Africa. Smart-city platforms built by Huawei and Alibaba, surveillance systems incorporating Chinese AI, and telecommunications infrastructure carrying Chinese technical standards extend China's cyber governance model far beyond its borders.\autocite{roberts2021chinese} The result is a de facto `Beijing Effect' in AI governance: developing nations that adopt Chinese-built AI infrastructure implicitly adopt the data governance and content-control frameworks embedded within it, often without the explicit legislative adoption that characterises the Brussels Effect.

The foundational layer of this ecosystem predates AI-specific regulation by several years. The 2017 Cybersecurity Law, the 2021 Data Security Law, and the Personal Information Protection Law (PIPL) established a comprehensive data governance architecture, covering collection, storage, cross-border transfer, and security, within which AI-specific regulations now operate.\autocite{roberts2021chinese} The CAC's algorithm registration system, which requires companies to disclose the logic and parameters of recommendation algorithms, functions simultaneously as an AI governance mechanism and a tool for ensuring that algorithmic systems do not undermine social stability or state information-control objectives. For international firms, compliance with China's AI regulations necessarily entails compliance with its broader cybersecurity and data-localisation requirements, a coupling that neither the EU nor the US imposes with comparable institutional force.

\subsection{Recent Developments (2025)}

Two developments in 2025 deepened the integration of AI governance within China's cybersecurity architecture. In August, the State Council issued the `AI Plus' Action Plan,\autocite{china_aiplus_2025} an industrial policy targeting 70\% AI penetration across key economic sectors by 2027, framing AI adoption as a matter of national competitiveness. More consequentially for cyber governance, the National People's Congress amended the Cybersecurity Law in October\autocite{china_cybersecurity_2025} to incorporate AI governance provisions for the first time in primary national legislation. The amendments mandate AI ethics review and risk assessment for systems deployed in critical infrastructure, increase administrative penalties tenfold (from RMB~1~million to RMB~10~million), and grant the CAC expanded enforcement authority over AI systems that process personal data or affect network security. Mandatory AI content-labelling rules also took effect in September 2025. The institutional logic is clear: AI governance is not a separate policy domain but an extension of the cybersecurity and data-sovereignty framework that the CAC already administers.

% ──────────────────────────────────────────────
\section{United States}
% ──────────────────────────────────────────────

\subsection{Overview}

On 30 October 2023, President Biden signed Executive Order~\#14110 on the `Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence'.\autocite{whitehouse2023b} EO~\#14110 delegated AI responsibilities to over fifty existing federal agencies with over one hundred specific tasks, moving beyond the voluntary commitments secured in July 2023\autocite{whitehouse2023a} and the October 2022 AI Bill of Rights.\autocite{whitehouse2022}

\subsection{Laws and Regulations}

The US regulatory tradition emphasises decentralised oversight: Congress passes framework legislation, and specialised federal agencies enforce rules within their domains.\autocite{nsf2024}\autocite{sec2024}\autocite{epa2024} Technology regulation has historically combined legislative action, executive orders, agency rulemaking, and industry self-regulation.\autocite{cusumano2021}\autocite{minow2023} This distributed approach reflects both philosophical distrust of centralised power and the practical influence of a \$46~billion lobbying industry.\autocite{massoglia2024}

EO~\#14110 was somewhat exceptional: the executive branch initiated AI-related policies partly due to its ability to respond more quickly than Congress. In contrast to the more centralised approaches of the EU and China, the order nonetheless reflected the distinct US approach of delegating specific objectives to agencies with pre-existing domain expertise.\autocite{perkins2024}

\subsection{Executive Order 14110}

\FloatBarrier

EO~\#14110 directed over fifty federal agencies to take over one hundred specific actions addressing eight core policy areas including safety and security, innovation and competition, worker support, bias and civil rights, consumer protection, privacy, federal use of AI, and international leadership.\autocite{whitehouse2024a} Both the 180-day and 270-day deadlines were met.\autocite{whitehouse2024b}\autocite{whitehouse2024d}

EO~\#14110 addressed many core concerns highlighted in the EU AI Act but with key differences.\autocite{crs2024} Where the EU established a new centralised authority (the AI Office), the US augmented existing federal agencies. The US approach was arguably more immediately actionable but also more vulnerable to political reversal, a vulnerability dramatically demonstrated in January 2025.

Within this structure, the National Institute of Standards and Technology (NIST) established the US AI Safety Institute in January 2024,\autocite{nist2024a} with task forces focusing on safety, evaluation, measurement, and risk management.\autocite{nist2024b}

\subsection{AI and Cybersecurity Governance}

Unlike the EU's integrated approach through the AI Act and NIS2, the United States lacks a unified framework connecting AI governance with cybersecurity policy. Before its revocation, EO~\#14110 assigned cybersecurity-related AI tasks to multiple agencies: NIST was directed to develop AI red-teaming standards and adversarial testing guidelines; the Department of Homeland Security (DHS) was tasked with assessing AI-related threats to critical infrastructure; and the Cybersecurity and Infrastructure Security Agency (CISA) published its `Roadmap for Artificial Intelligence' in late 2023, identifying AI as both an essential tool for cyber defence and a vector for novel threats. Meanwhile, the Department of Defence's Replicator initiative committed to fielding autonomous AI-enabled systems at scale, raising governance questions about autonomous cyber operations that no civilian regulatory framework addresses.

Spread across NIST, CISA, DHS, DOD, and sector-specific agencies, this distributed approach produced rapid progress under EO~\#14110's coordination mandate. But it also created a structural vulnerability: because coordination depended on presidential direction rather than legislative authority, the entire framework was susceptible to political reversal.

\subsubsection{Revocation and Its Cyber Governance Consequences}

It was realised on 20~January 2025, when President Trump revoked EO~\#14110 on his first day in office. His replacement order, EO~14179 (`Removing Barriers to American Leadership in Artificial Intelligence'),\autocite{trump2025eo14179} establishes a policy of sustaining US AI dominance through economic competitiveness and national security, but contains no cybersecurity requirements, inter-agency coordination mandates, safety benchmarks, or compliance deadlines. The 180-day AI Action Plan it mandates had, as of late 2025, not addressed the cybersecurity dimensions that EO~\#14110 had begun to operationalise.

The consequences for US cyber governance are immediate. CISA's AI Roadmap lacks binding authority without executive-level backing. NIST's AI Safety Institute continues to operate but with a significantly narrowed mandate. The result is a governance vacuum in which AI-cybersecurity coordination depends on agency initiative rather than presidential direction, precisely as AI-enabled cyber threats are proliferating. This vacuum is a stark expression of the US position in the trilemma: by prioritising innovation speed through deregulation, the Trump administration has simultaneously weakened the coherence of federal AI-cybersecurity governance.

\subsection{California SB~1047 and State-Level Regulation}

\FloatBarrier

California Senate Bill 1047, introduced in February 2024, attempted to establish a regulatory framework for `frontier models' defined by computational resources.\autocite{LegiScan2024} The bill attracted both broad support and vigorous opposition.\autocite{thehill_ai_supporters_2024}\autocite{nunez2024} Governor Newsom vetoed SB~1047 on 29 September 2024\autocite{newsom2024veto} because he opposed standards solely based on model size, favouring instead assessment of deployment contexts. The episode shows the gravitational pull of institutional traditions: even ambitious ex ante proposals get reshaped by the US system's preference for ex post, risk-based approaches.

The SB~1047 debate catalysed a wave of state-level AI legislation in 2025. California enacted SB~53, the Transparency in Frontier Artificial Intelligence Act,\autocite{casb53_2025} on 29 September 2025. SB~53 shifts from prescriptive safety mandates to a transparency-centred approach: developers of frontier models (trained at $>10^{26}$ operations) must publish annual risk-governance frameworks, issue pre-deployment transparency reports, and report critical safety incidents within 15 days. New York followed with the RAISE Act\autocite{ny_raise_2025} in December 2025, requiring annual independent audits and 72-hour incident reporting. Together, these laws suggest that US AI governance is settling on transparency and disclosure obligations rather than ex ante safety requirements.

% ──────────────────────────────────────────────
\section{Discussion: The Cyber Governance Trilemma}
% ──────────────────────────────────────────────

\subsection{Comparative Synthesis}

Applying the governance typology introduced in Section~2, a clear pattern emerges. The EU occupies the centralised, ex ante quadrant: the AI Act establishes a unified, risk-based framework that regulates AI systems \textit{before} they reach the market, enforced through a new centralised authority. The US occupies the distributed, ex post quadrant: even before the revocation of EO~\#14110, the US relied heavily on industry self-regulation and existing legal frameworks. China occupies a hybrid position: centralised guidance and registration requirements coexist with decentralised, sector-specific enforcement and deliberate regulatory forbearance for innovative SMEs.

These divergent positions produce what we term the \textit{cyber governance trilemma}. Drawing on the international political economy literature on `impossible trinities', we argue that jurisdictions cannot simultaneously optimise three desirable objectives:

\begin{enumerate}
  \item \textbf{Innovation speed}: minimising regulatory barriers to AI development and deployment.
  \item \textbf{Safety and rights protection}: ensuring AI systems do not cause harm, discrimination, or rights violations.
  \item \textbf{Regulatory interoperability}: maintaining compatibility with other jurisdictions' frameworks to enable cross-border operations.
\end{enumerate}

The trilemma is not merely an empirical coincidence---it reflects a structural incompatibility among the three objectives, operating through three bilateral tensions.

The \textit{innovation--safety tension} is the most widely discussed: stringent ex ante regulation requires developers to demonstrate compliance before deployment, adding time and cost that directly reduce innovation speed. The EU's conformity assessment regime for high-risk AI systems illustrates this clearly: providers must complete risk assessments, establish quality management systems, and maintain technical documentation before market access, a process that delays deployment relative to jurisdictions without such requirements. Conversely, permissive regulation accelerates deployment but increases harm probability, as the US experience with largely unregulated AI-enabled content recommendation systems and deepfake generation tools demonstrates.

The \textit{safety--interoperability tension} is less intuitive but equally consequential. Stronger domestic safety standards inevitably diverge from other jurisdictions' definitions of acceptable risk, creating regulatory walls that impede cross-border AI operations. China's content-labelling requirements for AI-generated material are structurally incompatible with EU transparency standards, which focus on system-level risk classification rather than content-level marking. The more rigorously a jurisdiction specifies safety requirements, and the more those requirements reflect domestic political values, the greater the divergence from jurisdictions with different risk tolerances.

The \textit{innovation--interoperability tension} operates through the Brussels Effect mechanism itself. Establishing global regulatory standards requires comprehensive, prescriptive rules with extraterritorial reach; but such rules constrain domestic innovators who must comply with standards designed for international projection. The EU's GPAI provisions have drawn criticism for imposing compliance burdens on European open-source AI developers that their US and Chinese competitors do not face, potentially deterring the very innovation that global standard-setting requires.

The structural cause of the trilemma is that AI governance involves distributional choices---who bears risk, who captures economic value, who sets international norms---that map onto incompatible political economy preferences across jurisdictions. No single regulatory design can simultaneously minimise barriers to innovation, maximise safety guarantees, and maintain cross-border regulatory compatibility, because each objective demands institutional choices that work against the other two.

Each jurisdiction therefore sacrifices one objective, as summarised in Table~\ref{tab:trilemma}. The EU prioritises safety and interoperability (through its Brussels Effect ambitions) at the cost of innovation velocity. The US, particularly after the revocation of EO~\#14110, prioritises innovation speed at the cost of consistent safety guarantees. China prioritises innovation speed and selective safety enforcement at the cost of international interoperability, embedding AI governance within a cyber sovereignty strategy that is structurally incompatible with multi-stakeholder governance models.\autocite{mueller2017internet}

\begin{table}[ht]
\centering\small
\begin{tabularx}{\linewidth}{@{}l c c c@{}}
\toprule
\textbf{Jurisdiction} & \textbf{Innovation} & \textbf{Safety/Rights} & \textbf{Interoperability} \\
\midrule
European Union & Constrained & Strong (ex ante) & High (Brussels Effect) \\
United States & Prioritised & Weak (post-EO revocation) & Moderate \\
China & Prioritised (selective) & Selective enforcement & Low (cyber sovereignty) \\
\bottomrule
\end{tabularx}
\caption{The cyber governance trilemma: how each jurisdiction resolves the innovation--safety--interoperability trade-off.}
\label{tab:trilemma}
\end{table}

The Brussels Effect thesis receives only partial support from this analysis. The EU AI Act's extraterritorial reach creates compliance incentives for global firms, but the simultaneous emergence of substantive frameworks in both the US and China limits the unilateral standard-setting power that the EU enjoyed with the GDPR. AI governance is evolving towards a \textit{multipolar regulatory landscape}\autocite{smuha2021race} in which firms must navigate competing compliance regimes, a pattern that mirrors the broader fragmentation of internet governance.\autocite{nye2014regime}

\subsection{Implications for International Cyber Governance}

Regulatory fragmentation carries several concrete implications for the international cyber governance community.

\textit{Regulatory fragmentation as systemic cyber risk.} Divergent AI governance frameworks create compliance gaps: seams between regulatory regimes that well-resourced threat actors can exploit. Where AI systems in critical infrastructure must satisfy different safety standards depending on jurisdiction, the weakest link determines the overall cyber resilience of interconnected systems. The interaction between the EU's AI Act and NIS2 Directive illustrates the emerging model of integrated AI-cybersecurity governance, but no comparable integration exists at the international level.

\textit{Implications for the transatlantic relationship and China.} The Trump administration's revocation of EO~\#14110 widened the regulatory gap between the US and EU precisely as the AI Act entered enforcement. European firms operating in the US face declining regulatory certainty, while US firms must still comply with the AI Act for EU market access. Meanwhile, China's integration of AI governance into its Cybersecurity Law creates a self-contained regulatory ecosystem that reinforces cyber sovereignty, complicating already-fraught US--China technology competition and multilateral cyber norm processes.

\textit{Model adoption by developing nations.} For developing countries that lack the institutional capacity to design bespoke AI governance frameworks, the choice between regulatory models carries significant geopolitical implications.\autocite{denardis2014global}\autocite{carr2016public} The GDPR experience, in which over 130 countries adopted data protection laws influenced by the EU model, suggests that many countries will align with one of the three major AI regulatory frameworks. Early evidence confirms this pattern but also reveals more complex dynamics than simple emulation.

Brazil's AI regulatory framework, approved by the Senate in December 2024, draws heavily on the EU AI Act's risk-based classification system while incorporating provisions for algorithmic impact assessments tailored to the country's acute concerns about racial and socioeconomic discrimination. India's proposed Digital India Act takes a different path, emphasising light-touch regulation and industry self-governance that more closely resembles the US approach, reflecting India's ambition to position itself as a global AI development hub. Singapore's Model AI Governance Framework, now in its third iteration, represents a pragmatic middle ground: voluntary guidelines grounded in risk assessment and transparency principles, designed to attract international AI investment without imposing compliance burdens that would deter firms from establishing regional headquarters.

At the multilateral level, the African Union's Continental AI Strategy (2024) and the ASEAN Guide on AI Governance and Ethics both signal a preference for principles-based frameworks that preserve regulatory flexibility; but the practical governance choices of individual member states are increasingly shaped by which major power provides their AI infrastructure. For cyber governance, this `regulatory template' dynamic is consequential: developing nations' choices about AI regulation determine their alignment in international cyber norm debates, including their positions in the UN Open-Ended Working Group on Information and Communications Technologies and the Global Digital Compact. The trilemma thus operates not only within the three major jurisdictions but across the entire global governance architecture, as regulatory choices cascade through investment relationships, infrastructure dependencies, and multilateral alignments.

\textit{Implications for international norm-building.} The trilemma complicates ongoing efforts to establish international AI governance norms through the United Nations and other multilateral forums. If the three major AI powers cannot agree on basic regulatory approaches (ex ante versus ex post, centralised versus distributed, rights-based versus innovation-first), the prospects for a binding international AI governance regime remain limited. More realistic pathways may include issue-specific agreements (on autonomous weapons, deepfakes, or AI-enabled cyber operations), mutual recognition frameworks, or informal convergence through regulatory competition.

\subsection{AI Governance and Cyber Conflict}

The cyber governance trilemma extends beyond civilian AI regulation into the military and security domain, where the stakes are highest and the governance gaps most consequential. AI-enabled capabilities are transforming cyber operations across all three jurisdictions: automated vulnerability discovery, AI-generated spear-phishing at scale, autonomous network penetration tools, and machine-speed decision-making in cyber defence all blur the boundary between civilian AI governance and military cyber operations.\autocite{kello2017virtual}

Each jurisdiction's regulatory framework handles this boundary differently, and problematically. The EU AI Act explicitly exempts national security and defence applications from its scope, creating a regulatory gap in which military AI systems face no comparable governance framework at the European level. China's military-civil fusion strategy deliberately erases the distinction between civilian and military AI development: technologies developed under civilian AI regulations are systematically channelled into People's Liberation Army modernisation programmes, meaning that China's ostensibly civilian AI governance framework has direct military implications that neither the EU nor the US regulatory frameworks account for. The United States, through programmes such as the Department of Defence's Replicator initiative and DARPA's AI-enabled cyber operations research, is developing autonomous systems that operate in a governance grey zone between EO~14179's civilian innovation mandate and the Laws of Armed Conflict.

The trilemma applies to military AI governance with particular force. No jurisdiction can simultaneously pursue AI-enabled military advantage (innovation), compliance with international humanitarian law and arms-control norms (safety), and interoperability with allies' autonomous systems (interoperability). The UN Group of Governmental Experts on Lethal Autonomous Weapons Systems and the Open-Ended Working Group on ICT security have both struggled to establish meaningful norms precisely because the three major AI powers approach military AI governance from incompatible positions. Issue-specific agreements (on autonomous cyber weapons, AI-generated disinformation in armed conflict, or machine-speed escalation dynamics) may represent the most achievable near-term pathway, but even these require a degree of regulatory convergence that the trilemma structurally impedes.

% ──────────────────────────────────────────────
\section{Conclusion}
% ──────────────────────────────────────────────

The EU, China, and the United States are constructing fundamentally different AI regulatory systems, each reflecting distinct institutional traditions, political values, and geopolitical ambitions. The resulting regulatory fragmentation is not merely an inconvenience for global firms---it is a structural feature of cyber governance that shapes cybersecurity, international cooperation, and the distribution of technological power. As AI capabilities advance and become more deeply embedded in critical systems, the trilemma will only sharpen. How different jurisdictions resolve the tensions between innovation, safety, and interoperability will determine the shape of international cyber governance for decades to come.

% ──────────────────────────────────────────────
\section*{Author Contributions}
% ──────────────────────────────────────────────
Jon Chun led the comparative analysis of the United States and China, the cyber governance trilemma framework, and the regulatory timeline dataset. Christian Schroeder de Witt led the analysis of the European Union and the regime complexity framing. Katherine Elkins contributed to the United States analysis and the developing-nations implications. All authors contributed to the theoretical framework, methodology, and comparative synthesis.

% ──────────────────────────────────────────────
\section*{About the Authors}
% ──────────────────────────────────────────────

\textbf{Jon Chun} is Visiting Instructor of Humanities at Kenyon College, with graduate degrees in computer science and electrical engineering from UC Berkeley and UT Austin and extensive industry experience in FinTech and cybersecurity. He is lead investigator for the Modern Language Association's participation in the NIST US AI Safety Institute and co-principal investigator on an IBM--Notre Dame Tech Ethics Lab grant examining AI decision-making in criminal justice contexts. His research encompasses human-centred AI, AI safety, and technology policy.

\textbf{Christian Schroeder de Witt} is Lecturer in the Department of Engineering Science at the University of Oxford and Principal Investigator of the Oxford Witt Lab for Trust in AI (OWL). A Royal Academy of Engineering Research Fellow and Schmidt Sciences AI2050 Fellow, his research addresses multi-agent security and AI assurance, examining how decentralised AI systems can be secured against emerging threats. He serves as an expert adviser to RAND, the BBC, and the UK government on AI governance and risk.

\textbf{Katherine Elkins} is Professor of Comparative Literature and Humanities at Kenyon College, with affiliated faculty status in Computing. She co-founded Kenyon's human-centred AI curriculum in 2016 and serves as co-principal investigator for both the NIST US AI Safety Institute and the IBM--Notre Dame Tech Ethics Lab grant on AI decision-making in recidivism cases. Her research bridges computational methods with humanistic inquiry to inform policy on AI safety, bias, and governance.

% ──────────────────────────────────────────────
% Endnotes — Chatham House numbered endnotes
% ──────────────────────────────────────────────
\FloatBarrier
\clearpage
\begingroup
\parindent 0pt
\parskip 0.5\baselineskip
\theendnotes
\endgroup

\end{document}
