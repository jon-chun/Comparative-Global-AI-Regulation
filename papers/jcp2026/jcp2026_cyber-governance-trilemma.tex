\documentclass[a4paper,12pt]{article}

% ──────────────────────────────────────────────
% Font — Times New Roman (JCP / Taylor & Francis)
% ──────────────────────────────────────────────
\usepackage[T1]{fontenc}
\usepackage{mathptmx}   % Times-compatible body text and maths

% ──────────────────────────────────────────────
% Page layout — A4, 2.5 cm margins, double spacing
% ──────────────────────────────────────────────
\usepackage[a4paper, margin=2.5cm]{geometry}
\usepackage[doublespacing]{setspace}
\usepackage[parfill]{parskip}

% ──────────────────────────────────────────────
% Section headings — bold, serif (Times)
% ──────────────────────────────────────────────
\usepackage{titlesec}
\titleformat{\section}{\Large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\large\bfseries}{\thesubsection}{1em}{}
\titleformat{\subsubsection}{\normalsize\bfseries}{\thesubsubsection}{1em}{}

% ──────────────────────────────────────────────
% Notes — Chatham House numbered endnotes
% ──────────────────────────────────────────────
\usepackage{endnotes}
\let\footnote=\endnote
\renewcommand{\notesname}{Notes}

% ──────────────────────────────────────────────
% Bibliography — biblatex verbose-note for footnote-style citations
% ──────────────────────────────────────────────
\usepackage[hyphens]{url}
\usepackage{csquotes}
\usepackage[
  style=verbose-note,
  backend=biber,
  abbreviate=true,
  ibidtracker=false
]{biblatex}
\addbibresource{jcp2026_cyber-governance-trilemma.bib}

% Remap non-standard entry types to @misc
\DeclareSourcemap{
  \maps[datatype=bibtex]{
    \map{
      \step[typesource=legal, typetarget=misc]
    }
    \map{
      \step[typesource=legislation, typetarget=misc]
    }
    \map{
      \step[typesource=report, typetarget=misc]
    }
  }
}

% ──────────────────────────────────────────────
% Figures and tables
% ──────────────────────────────────────────────
\usepackage{graphicx}
\usepackage{float}
\usepackage{placeins}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{longtable}
\usepackage{array}
\usepackage{adjustbox}

% ──────────────────────────────────────────────
% Hyperlinks and URLs
% ──────────────────────────────────────────────
\usepackage{xcolor}
\usepackage[breaklinks,colorlinks,linkcolor=blue!70!black,citecolor=blue!70!black,urlcolor=blue!70!black]{hyperref}
\usepackage{bookmark}

% ──────────────────────────────────────────────
% Miscellaneous
% ──────────────────────────────────────────────
\usepackage{enumitem}

\DeclareUnicodeCharacter{2060}{}

% ──────────────────────────────────────────────
% Metadata
% ──────────────────────────────────────────────
\title{\textbf{The Cyber Governance Trilemma:\\Comparative AI Regulation in the EU, China,\\and the United States}}

\author{
    Jon Chun\textsuperscript{1} \\
    Christian Schroeder de Witt\textsuperscript{2} \\
    Katherine Elkins\textsuperscript{1} \\
    \\
    \textsuperscript{1}Kenyon College \\
    \textsuperscript{2}University of Oxford
}

\date{}  % JCP provides its own date in production

% ══════════════════════════════════════════════
\begin{document}
% ══════════════════════════════════════════════

\maketitle

% ──────────────────────────────────────────────
\begin{abstract}
Artificial intelligence regulation now poses one of the central challenges of international cyber governance. As AI systems are deployed across critical infrastructure and cybersecurity operations, the regulatory regimes governing their development shape global cyber policy in ways that few anticipated a decade ago. We compare the three most influential AI regulatory approaches: the EU AI Act, China's sector-specific regulations, and the evolving US approach from Executive Order~\#14110 through its revocation and state-level initiatives including California's SB~53. Drawing on regulatory competition theory, a governance typology from comparative regulatory capitalism, and regime complexity, we argue that these three jurisdictions face a \textit{cyber governance trilemma}: no jurisdiction can simultaneously optimise innovation speed, safety and rights protection, and regulatory interoperability. The EU accepts slower innovation in exchange for strong safety guarantees; the US, especially after revoking EO~\#14110, has moved in the opposite direction, favouring speed over regulatory consistency. China pursues state-directed competitiveness but at the price of international interoperability. This regulatory fragmentation is itself a cyber risk: it creates compliance gaps that sophisticated threat actors can exploit and complicates international norm-building. With developments current through late 2025, the paper concludes with implications for international cyber governance and for developing nations caught between competing regulatory models.
\end{abstract}

\medskip
\noindent\textbf{Keywords:} AI regulation, cyber governance, regulatory fragmentation, EU AI Act, comparative policy, Brussels Effect

% ──────────────────────────────────────────────
\section{Introduction}
% ──────────────────────────────────────────────

The regulation of artificial intelligence can no longer be separated from cyber governance. AI-generated deepfakes undermine election integrity and enable sophisticated social engineering attacks. Autonomous agents probe critical infrastructure for vulnerabilities at machine speed, and adversarial actors exploit large language models to generate polymorphic malware that evades signature-based defences.\autocite{kello2017virtual} AI systems have also become integral to cyber defence, whether for anomaly detection in financial networks or automated threat intelligence. How a jurisdiction regulates AI therefore determines the safety of its own digital infrastructure and the security posture of every interconnected system.\autocite{nye2014regime}

As jurisdictions race to regulate these dual-use capabilities, the resulting patchwork of divergent rules creates a new form of systemic cyber risk: regulatory fragmentation. Cybersecurity governance once centred on technical standards and incident response.\autocite{broeders2017public} The challenge now goes further: how should societies govern AI development and deployment? The divergent answers from the EU, China, and the United States have implications both for domestic technology policy and for the architecture of international cyber governance. Gaps between regulatory regimes create seams that well-resourced threat actors can exploit: an AI system that satisfies Chinese content-labelling requirements may violate EU transparency standards, yet a system compliant with the EU AI Act's high-risk provisions may be entirely unregulated under the post-2025 US approach.

A growing body of scholarship has examined how different jurisdictions are approaching AI governance. Early comparative work mapped the ethical principles underpinning national AI strategies in the US, EU, and UK,\autocite{cath2018artificial}\autocite{floridi2018ai4people} Stix identified three distinct policy pathways (ethics-based, rights-based, and risk-based) that governments have pursued,\autocite{stix2021actionable} and Smuha documented the emergence of `regulatory competition' in AI, arguing that jurisdictions now race to regulate AI as much as to develop it.\autocite{smuha2021race} Studies have examined individual regimes in depth, including the EU AI Act,\autocite{veale2021demystifying} China's layered regulatory approach,\autocite{roberts2021chinese} and the framing contests that shape AI policy.\autocite{ulnicane2021framing} Yet few studies offer a systematic three-way comparison that situates the EU, China, and the US within a unified analytical framework, and fewer still examine AI regulation through the lens of cyber governance. We address that gap.

Much has changed since 2024. The EU AI Act's prohibitions on high-risk AI practices took effect in February 2025. In the United States, President Trump revoked Executive Order~\#14110 on his first day in office in January 2025, California enacted a successor transparency law (SB~53) in September 2025, and New York passed the RAISE Act in December 2025. China amended its Cybersecurity Law in October 2025 to incorporate AI governance provisions for the first time in national legislation. AI regulation is evolving rapidly and unevenly, with consequences for the coherence of international cyber governance that are only beginning to be understood.

We make three contributions. First, we provide a systematic comparative analysis of AI regulatory approaches across the three largest AI economies, updated through late 2025. Second, we introduce a \textit{cyber governance trilemma}: the proposition that jurisdictions cannot simultaneously optimise innovation speed, safety and rights protection, and regulatory interoperability. Third, we draw out implications for international cyber governance, including for developing nations caught between competing regulatory models.

% ──────────────────────────────────────────────
\section{Theoretical Framework}
% ──────────────────────────────────────────────

A substantial literature now documents the divergent paths that jurisdictions follow when they regulate emerging technologies, paths shaped by institutional legacies, political economy, and competing visions of the state's role in cyberspace.\autocite{cath2018artificial}\autocite{ulnicane2021framing} Smuha has shown that the competition to set the rules governing AI has become at least as consequential as the competition to build the technology.\autocite{smuha2021race} We draw on four theoretical perspectives to move beyond description.

First, we draw on \textit{regulatory competition theory}, which holds that jurisdictions compete, and learn from one another, through their regulatory choices.\autocite{bradford_brussels_2020} Bradford's concept of the `Brussels Effect' describes a mechanism by which the EU's large single market incentivises global firms to adopt EU standards unilaterally. We examine whether the EU AI Act exhibits this dynamic and how competing regulatory models may limit it.

Second, we use a \textit{governance typology} that distinguishes regulatory approaches along two dimensions: (1)~the degree of \textit{centralisation} (top-down versus distributed) and (2)~the primary \textit{regulatory modality} (ex ante risk-based rules versus ex post enforcement through existing legal frameworks). The typology draws on Majone's analysis of the `regulatory state' in Europe\autocite{majone1994rise} and Levi-Faur's account of the `global diffusion of regulatory capitalism',\autocite{levifaur2005global} applying their comparative insights to the AI domain. Applied to AI, the typology places the EU squarely in the centralised, ex ante quadrant, but the US leans towards distributed, ex post enforcement. China combines elements of both: centralised guidance paired with decentralised, sector-specific implementation.

Third, every AI governance framework must manage the tension between promoting innovation and safeguarding against harm, a tradeoff that Stix identifies as a defining axis of contemporary AI policy.\autocite{stix2021actionable} The three jurisdictions resolve it in characteristically different ways. The EU embeds precaution and fundamental-rights protection into binding ex ante rules;\autocite{veale2021demystifying} China yokes social-stability imperatives to state-directed industrial strategy;\autocite{roberts2021chinese} and the US defaults to market-led innovation, relegating safety largely to voluntary industry commitments.

Fourth, we draw on Nye's concept of \textit{regime complexity}\autocite{nye2014regime} and the broader literature on internet governance fragmentation\autocite{mueller2017internet}\autocite{denardis2014global} to situate AI regulation within the evolving architecture of cyber governance. The absence of a single overarching regime for AI governance mirrors the fragmented regime complex that characterises cyberspace more broadly, and it complicates both international cooperation and the management of transboundary cyber risks.\autocite{eichensehr2019cyber}

% ──────────────────────────────────────────────
\section{Methodology}
% ──────────────────────────────────────────────

We use a comparative case study methodology to examine how AI regulatory regimes interact with, and reshape, cyber governance across three jurisdictions: the European Union, the People's Republic of China, and the United States (at both federal and state levels). Case selection follows a most-different-systems design: the three cases represent the world's largest AI economies while instantiating distinct governance traditions: centralised ex ante regulation, hybrid state-guided governance, and distributed market-oriented oversight. The design allows identification of underlying factors that drive regulatory divergence and its cyber governance consequences.

Our analytical approach combines \textit{legal and policy document analysis} with \textit{regime complexity mapping}. For each jurisdiction, we examined the principal legislative texts, executive orders, regulatory guidance, and enforcement actions current through late 2025, with particular attention to provisions governing AI systems in cybersecurity-relevant domains: critical infrastructure, defence and intelligence, surveillance, and information integrity. We supplement primary legal sources with peer-reviewed scholarship and policy institute reports, as well as practitioner commentary. The comparison is structured around the governance typology and regime complexity concepts introduced above, with the comparative regulatory timeline (Section~4) serving as an original empirical anchor for tracking the pace and competitive dynamics of regulatory activity across jurisdictions.

% ──────────────────────────────────────────────
\section{Comparative Regulatory Timeline}
% ──────────────────────────────────────────────

Table~\ref{tab:timeline} presents a comparative regulatory timeline constructed as a structured dataset of AI governance milestones across the three jurisdictions. Inclusion criteria were: (1)~binding legislation, executive orders, or regulations with direct AI governance provisions; (2)~institutional actions establishing new AI-specific governance bodies; and (3)~enforcement milestones marking the operational commencement of regulatory obligations. Non-binding strategies, voluntary commitments, and subnational actions below the state or provincial level were excluded unless they demonstrably shaped national regulatory trajectories (as California's SB~1047 debate did for US AI governance).

\begin{table}[ht]
\centering\small
\begin{tabularx}{\linewidth}{@{}l l X@{}}
\toprule
\textbf{Date} & \textbf{Jurisdiction} & \textbf{Milestone} \\
\midrule
Nov 2016 & China & Cybersecurity Law adopted (effective June 2017) \\
Apr 2018 & EU & AI Strategy: \textit{Artificial Intelligence for Europe} \\
Feb 2019 & US & Executive Order 13859 on maintaining US leadership in AI \\
Apr 2021 & EU & AI Act proposed by European Commission \\
Sept 2021 & China & Data Security Law takes effect \\
Nov 2021 & China & Personal Information Protection Law (PIPL) takes effect \\
Mar 2022 & China & Algorithm Recommendation Regulation takes effect \\
Oct 2022 & US & Blueprint for an AI Bill of Rights \\
Jan 2023 & China & Deep Synthesis Regulation takes effect \\
Aug 2023 & China & Generative AI Measures take effect \\
Oct 2023 & US & Biden signs EO~\#14110 (100+ delegated tasks to 50+ agencies) \\
Dec 2023 & EU & AI Act provisionally agreed (European Parliament + Council) \\
Jan 2024 & US & NIST establishes US AI Safety Institute \\
Aug 2024 & EU & AI Act enters into force \\
Sept 2024 & US & Governor Newsom vetoes California SB~1047 \\
Jan 2025 & US & Trump revokes EO~\#14110; signs EO~14179 (3 days later) \\
Feb 2025 & EU & Prohibited AI practices take effect \\
Aug 2025 & EU & GPAI model governance obligations take effect \\
Aug 2025 & China & State Council issues `AI Plus' Action Plan \\
Sept 2025 & US & California enacts SB~53 (Transparency in Frontier AI Act) \\
Oct 2025 & China & NPC amends Cybersecurity Law to incorporate AI governance \\
Dec 2025 & US & New York enacts RAISE Act \\
Aug 2026 & EU & High-risk AI system requirements take effect (scheduled) \\
\bottomrule
\end{tabularx}
\caption{Comparative timeline of major AI regulatory milestones (2016--2026).}
\label{tab:timeline}
\end{table}

Three patterns stand out. Most striking is the acceleration: only three entries fall before 2021, but fifteen fall between 2023 and 2026, a compression that tracks the post-ChatGPT urgency across all three jurisdictions. Equally notable is divergent sequencing. China led with targeted, sector-specific regulations (algorithmic recommendation, deepfakes, generative AI) before attempting consolidation through Cybersecurity Law amendments in 2025. The EU pursued a single omnibus legislative instrument over a multi-year process, accepting slower adoption in exchange for systemic coherence. The US oscillated between executive action and state-level experimentation, with the revocation of EO~\#14110 representing a dramatic reversal in federal-level ambition. Finally, the timeline shows competitive clustering: between 2023 and 2025, all three jurisdictions undertook major regulatory actions within months of each other, consistent with the regulatory competition dynamics that Smuha theorised.\autocite{smuha2021race} This uncoordinated proliferation compounds the interoperability challenges that the trilemma describes.

% ──────────────────────────────────────────────
\section{The European Union}
% ──────────────────────────────────────────────

\subsection{Overview}

The 2024 EU AI Act is widely described as the world's first comprehensive AI law.\autocite{eu_ai_act_2024} Just as prior European general-purpose legislation, such as the 2016 General Data Protection Regulation (GDPR),\autocite{gdpr_article7} the EU AI Act was a joint product of the European Commission, Parliament, and Council. Influence on the Act's formation was taken publicly by national government officials, such as France's Macron lobbying for exemptions for open-source AI providers such as Mistral,\autocite{abboud_eus_2023} as well as by lobbying and industry groups, including Big Tech\autocite{perrigo_exclusive_2023} and German pro-open-source non-profit LAION.\autocite{noauthor_call_nodate}

Originally constructed within a product safety framework, the legislation was later blended with a fundamental rights agenda at the behest of the European Parliament.\autocite{caroli_podcast} This syncretism sets the EU AI Act apart from prior European legislation that built on established regimes such as the GDPR. It also follows earlier regulatory initiatives, including the 2022 Digital Markets Act\autocite{dma} and, of direct relevance to cyber governance, the NIS2 Directive on cybersecurity.\autocite{eu_nis2_2022} Together, the AI Act and NIS2 form the most tightly integrated AI-cybersecurity governance regime among the three jurisdictions examined here. The NIS2 Directive, which entered into force in January 2023 and required member-state transposition by October 2024, designates operators in energy, transport, banking, health, digital infrastructure, and public administration as `essential entities' subject to mandatory cybersecurity risk management and incident reporting. Where AI systems are deployed within these sectors (as they increasingly are, for grid management, fraud detection, medical diagnostics, and network traffic analysis) providers must satisfy both the AI Act's risk-based requirements and NIS2's cybersecurity obligations simultaneously. This regulatory overlap is by design: it reflects the EU's systemic approach to digital governance, in which AI safety, data protection (GDPR), and cybersecurity (NIS2) form an interlocking regulatory architecture.

\subsection{The Geopolitics of the Act}

Besides regulating the EU single market, the Act is often regarded as a strategic effort by the European Commission to establish themselves as the leading AI rulemakers globally.\autocite{noauthor_eus_2024} The logic is that companies across the world will prioritise compliance with European AI law out of economic necessity, not through coercion: the `Brussels Effect'.\autocite{bradford_brussels_2020}\autocite{almada_brussels_2024} One direct institutional consequence is the establishment of the EU AI Office, which will oversee AI regulation, provide a central pool of AI expertise to member states, and support a `strategic, coherent, and effective European approach on AI at the international level'.\autocite{eu_ai_office}

\subsection{Risk Classification and Regulation}

Designed as \textit{adaptive legislation}, the AI Act leaves details intentionally open for later specification. Its centrepiece is a risk classification system that places obligations primarily on the developers (`providers') of AI systems. The legislation applies both to systems placed on the EU market and to AI systems whose output is used in the EU, giving it considerable extraterritorial reach.

Four risk tiers structure the Act. \textit{Prohibited practices} include AI systems used for social scoring, manipulative subliminal techniques, and most real-time remote biometric identification.\autocite[Article 5]{eu_ai_act_2024} \textit{High-risk systems} constitute the majority of regulation, requiring risk management systems, data governance, human oversight, and cybersecurity measures.\autocite[Article 6]{eu_ai_act_2024} \textit{Limited-risk} systems face transparency obligations, while \textit{minimal-risk} systems are left unregulated.

From a cyber governance standpoint, the high-risk category is the most consequential. AI systems used as safety components of critical infrastructure (Annex~III, Area~2), AI systems used for law enforcement and border control (Annex~III, Areas~6--7), and AI systems deployed in democratic processes all fall within this tier. Article~15 of the Act explicitly requires providers of high-risk AI systems to achieve an `appropriate level of cybersecurity' proportionate to the risks, including resilience against adversarial manipulation and data poisoning as well as model extraction attacks, requirements that mirror and reinforce NIS2 obligations for essential entities. Neither the US nor China has linked AI regulation to cybersecurity standards this explicitly.

A separate category addresses \textit{general-purpose AI (GPAI) models}, with heightened obligations for those posing `systemic risk', defined as models trained with cumulative compute exceeding $10^{25}$ floating-point operations.\autocite{eu_ai_act_2024} Providers must register with the European Commission and adhere to wide-ranging safety and security criteria, including adversarial testing and model evaluation.

\subsection{Open Source, Innovation, and Implementation}

Wide-ranging exemptions exist for providers of AI systems under free and open-source software licences,\autocite[Article 53--54]{eu_ai_act_2024} provided they do not contain GPAI models of systemic risk. However, these exemptions are narrower than the de facto permissive environment in both the US and China, and the tension between protecting open-source development and regulating systemic risk remains unresolved.

By late 2025, the AI Act was no longer aspirational; it was being enforced. Prohibited-practices provisions became legally binding on 2~February 2025. GPAI model governance obligations followed on 2~August 2025, with penalties reaching \texteuro35~million or 7\% of global annual turnover for non-compliance. Full high-risk AI system requirements are scheduled for August 2026. In parallel, NIS2 transposition deadlines drove member states to integrate AI-specific cybersecurity obligations into national legislation, producing a synchronised rollout of AI safety and cybersecurity governance that neither the US nor China has attempted.

% ──────────────────────────────────────────────
\section{China}
% ──────────────────────────────────────────────

\subsection{Overview}

China's approach to AI governance is a hybrid: centralised strategic direction combined with decentralised, sector-specific implementation. Enforcement is selective by design. The central government sets overarching objectives, above all social stability and technological self-sufficiency; provincial governments and sectoral regulators then translate these into operational rules, often with considerable latitude for local experimentation.\autocite{zhang2022} The design is a calculated attempt to take the best of both the EU model (coherent top-down guidance) and the US model (market-driven innovation) without accepting the perceived weaknesses of either.

What distinguishes China from a cyber governance perspective is its institutional architecture. The Cyberspace Administration of China (CAC) is simultaneously the country's internet regulator, data protection authority, and primary AI governance body. This consolidation is deliberate: it ensures that AI regulation operates within, and reinforces, China's broader cyber sovereignty strategy, in which data governance, content control, algorithmic regulation, and cybersecurity operate as a single regulatory apparatus rather than separate policy domains.

\subsection{Laws and Regulations}

\FloatBarrier

China has advanced some of the first AI laws and regulations at the national level, summarised in Table~\ref{tab:china_laws}. Unlike the horizontal risk-based approach of the EU, China has favoured the sector-specific US approach of laws tailored to specific use cases. Despite appearances of centralised government control, Chinese AI regulations are the product of an iterative process involving diverse stakeholders including mid-level bureaucrats, academics, corporations, startups, and think tanks.\autocite{sheehan2024} The central government relies upon a pipeline of experts to formulate and interpret the details, while local officials mainly ensure alignment with Chinese and socialist ideology.\autocite{zhang2022}

\begin{table}[ht]
\centering\small
\begin{tabularx}{\linewidth}{@{}l >{\raggedright\arraybackslash}p{2.4cm} >{\raggedright\arraybackslash}p{1.8cm} >{\raggedright\arraybackslash}p{3.6cm} >{\raggedright\arraybackslash}p{3.6cm}@{}}
\toprule
\textbf{Date} & \textbf{Title} & \textbf{Issuing Body} & \textbf{Description} & \textbf{Cyber Governance Dimension} \\
\midrule
June 2017 & Cybersecurity Law & NPC & Legal frameworks for cybersecurity, data protection, and network security. & Foundation of China's cyber governance architecture; amended Oct 2025 to incorporate AI. \\
\addlinespace
Sept.\ 2021 & Data Security Law & NPC & Regulations on data processing and security affecting AI systems. & Data localisation and cross-border transfer controls; shapes AI training data governance. \\
\addlinespace
Nov.\ 2021 & PIPL & NPC & Comprehensive data privacy law governing personal information. & Requires consent for AI processing of personal data; extraterritorial provisions. \\
\addlinespace
Mar.\ 2022 & Algorithm Recommendation Reg. & CAC & Regulates recommendation algorithms, requiring transparency and fairness. & First algorithm-specific governance globally; algorithm registration as cyber governance tool. \\
\addlinespace
Jan.\ 2023 & Deep Synthesis Reg. & CAC & Governs generative AI authenticity and traceability. & Content-labelling mandates for AI-generated material; anti-disinformation function. \\
\addlinespace
Aug.\ 2023 & Generative AI Measures & CAC + 6 & Obligations on generative AI providers for legality and cybersecurity. & Mandatory security assessments; bridges AI governance and network security enforcement. \\
\bottomrule
\end{tabularx}
\caption{Chinese AI laws and regulations, with cyber governance dimensions.}
\label{tab:china_laws}
\end{table}

\FloatBarrier

\subsection{Registration, Compliance, and Industrial Policy}

On paper, China's AI regulation requirements are the most burdensome of the three jurisdictions. Model registration and data management rules illustrate how strict central regulation can slow innovation. As of March 2024, only 546 AI models had been registered, and just seventy were Large Language Models\autocite{chinamoney2024}, in stark contrast to the over 500,000 open-source LLMs on Huggingface.co,\autocite{huggingface2024} which is banned in China.\autocite{chinatalk2023}

Yet in practice, enforcement is selective by design. China's `Made In China 2025' industrial policy supports 10,000 `Little Giants', small and mid-sized enterprises recognised as key sources of innovation,\autocite{globaltimes2021} which are informally afforded regulatory leeway.\autocite{zhang2024} Startups and SMEs largely operate beneath the enforcement threshold as long as they lack a large public presence.\autocite{zhang2022} Selective enforcement is also accompanied by a growing network of municipal AI regulatory sandboxes. Shanghai and Shenzhen have both established AI pilot zones that allow companies to test novel AI applications under relaxed compliance requirements, mirroring the EU AI Act's sandbox provisions but with characteristically less formal structure. The Shanghai AI pilot zone, for instance, permits experimentation with autonomous driving and medical AI under simplified registration procedures, providing a controlled pathway for innovation that the national regulatory apparatus does not yet accommodate.

China's selective approach also has an international dimension. Through the Belt and Road Initiative's digital infrastructure investments, Chinese AI companies, and the regulatory norms embedded in their systems, are being exported to Southeast Asia, Central Asia, and Africa. Huawei's and Alibaba's smart-city platforms are the most visible channel, but surveillance systems and telecommunications infrastructure also carry Chinese technical standards abroad. The net effect is that China's cyber governance model now reaches well beyond its borders.\autocite{roberts2021chinese} The outcome is a de facto `Beijing Effect' in AI governance. Developing nations that adopt Chinese-built AI infrastructure implicitly adopt the data governance and content-control regimes embedded within it, often without the conscious legislative adoption that characterises the Brussels Effect.

This regulatory apparatus rests on a foundational layer that predates AI-specific regulation by several years. The 2017 Cybersecurity Law, the 2021 Data Security Law, and the Personal Information Protection Law (PIPL) established a broad data governance architecture covering collection, storage, and cross-border transfer, within which AI-specific regulations now operate.\autocite{roberts2021chinese} The CAC's algorithm registration system, which requires companies to disclose the logic and parameters of recommendation algorithms, doubles as an AI governance mechanism and a check against algorithmic systems that might undermine social stability or state information-control objectives. International firms that comply with China's AI regulations must therefore also comply with its broader cybersecurity and data-localisation requirements, a coupling that neither the EU nor the US imposes with comparable force.

\subsection{Recent Developments (2025)}

Two developments in 2025 tightened the link between AI governance and China's cybersecurity architecture. In August, the State Council issued the `AI Plus' Action Plan,\autocite{china_aiplus_2025} an industrial policy targeting 70\% AI penetration across key economic sectors by 2027, framing AI adoption as a matter of national competitiveness. Of greater consequence for cyber governance, the National People's Congress amended the Cybersecurity Law in October\autocite{china_cybersecurity_2025} to incorporate AI governance provisions for the first time in primary national legislation. The amendments mandate AI ethics review and risk assessment for systems deployed in critical infrastructure, increase administrative penalties tenfold (from RMB~1~million to RMB~10~million), and grant the CAC expanded enforcement authority over AI systems that process personal data or affect network security. Mandatory AI content-labelling rules also took effect in September 2025. The institutional logic is plain: AI governance is an extension of the cybersecurity and data-sovereignty apparatus that the CAC already administers, not a separate policy domain.

% ──────────────────────────────────────────────
\section{United States}
% ──────────────────────────────────────────────

\subsection{Overview}

On 30 October 2023, President Biden signed Executive Order~\#14110 on the `Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence'.\autocite{whitehouse2023b} The order delegated AI responsibilities to more than fifty existing federal agencies with more than one hundred specific tasks, moving beyond the voluntary commitments secured in July 2023\autocite{whitehouse2023a} and the October 2022 AI Bill of Rights.\autocite{whitehouse2022}

\subsection{Laws and Regulations}

In practice, the US regulatory tradition emphasises decentralised oversight: Congress passes framework legislation, and specialised federal agencies enforce rules within their domains.\autocite{nsf2024}\autocite{sec2024}\autocite{epa2024} Technology regulation has historically combined legislative action, executive orders, agency rulemaking, and industry self-regulation.\autocite{cusumano2021}\autocite{minow2023} This distributed approach reflects both philosophical distrust of centralised power and the practical influence of a \$46~billion lobbying industry.\autocite{massoglia2024} EO~\#14110 was somewhat exceptional in that the executive branch initiated AI-related policies partly because it could respond more quickly than Congress, but the order nonetheless reflected the US instinct for delegating specific objectives to agencies with pre-existing domain expertise.\autocite{perkins2024}

\subsection{Executive Order 14110}

\FloatBarrier

Its directives addressed eight policy areas including safety and security, innovation and competition, worker support, bias and civil rights, consumer protection, privacy, federal use of AI, and international leadership.\autocite{whitehouse2024a} Both the 180-day and 270-day deadlines were met.\autocite{whitehouse2024b}\autocite{whitehouse2024d}

EO~\#14110 addressed many of the same concerns as the EU AI Act, but through different institutional means.\autocite{crs2024} Where the EU established a new centralised authority (the AI Office), the US augmented existing federal agencies. The US approach was more immediately actionable but also more vulnerable to political reversal, as January 2025 would demonstrate. Within this structure, the National Institute of Standards and Technology (NIST) established the US AI Safety Institute in January 2024,\autocite{nist2024a} with task forces on safety, evaluation, measurement, and risk management.\autocite{nist2024b}

\subsection{AI and Cybersecurity Governance}

Unlike the EU's integrated approach through the AI Act and NIS2, the United States lacks a unified regime connecting AI governance with cybersecurity policy. Before its revocation, EO~\#14110 assigned cybersecurity-related AI tasks to multiple agencies. NIST was directed to develop AI red-teaming standards and adversarial testing guidelines. The Department of Homeland Security (DHS) received responsibility for assessing AI-related threats to critical infrastructure, while the Cybersecurity and Infrastructure Security Agency (CISA) published its `Roadmap for Artificial Intelligence' in late 2023, identifying AI as both an essential tool for cyber defence and a vector for novel threats. Meanwhile, the Department of Defence's Replicator initiative committed to fielding autonomous AI-enabled systems at scale, raising governance questions about autonomous cyber operations that no civilian regulatory regime addresses.

Spread across NIST, CISA, DHS, DOD, and sector-specific agencies, this distributed approach produced rapid progress under EO~\#14110's coordination mandate. But it also created a structural vulnerability: because coordination depended on presidential direction rather than legislative authority, the entire apparatus was susceptible to political reversal.

\subsubsection{Revocation and Its Cyber Governance Consequences}

It was realised on 20~January 2025, when President Trump revoked EO~\#14110 on his first day in office. His replacement order, EO~14179 (`Removing Barriers to American Leadership in Artificial Intelligence'),\autocite{trump2025eo14179} establishes a policy of sustaining US AI dominance through economic competitiveness and national security, but contains no cybersecurity requirements, inter-agency coordination mandates, safety benchmarks, or compliance deadlines. The 180-day AI Action Plan it mandates had, as of late 2025, not addressed the cybersecurity dimensions that EO~\#14110 had begun to operationalise.

For US cyber governance, the consequences are immediate. CISA's AI Roadmap lacks binding authority without executive-level backing, and NIST's AI Safety Institute continues to operate but under a narrowed mandate. What remains is a governance vacuum: AI-cybersecurity coordination now depends on agency initiative rather than presidential direction, even as AI-enabled cyber threats proliferate. By prioritising innovation speed through deregulation, the Trump administration has weakened the coherence of federal AI-cybersecurity governance, a concrete expression of the US position in the trilemma.

\subsection{California SB~1047 and State-Level Regulation}

\FloatBarrier

California Senate Bill 1047, introduced in February 2024, attempted to establish a regulatory regime for `frontier models' defined by computational resources.\autocite{LegiScan2024} The bill attracted both broad support and vigorous opposition.\autocite{thehill_ai_supporters_2024}\autocite{nunez2024} Governor Newsom vetoed SB~1047 on 29 September 2024\autocite{newsom2024veto} because he opposed standards solely based on model size, favouring instead assessment of deployment contexts. The episode illustrates the gravitational pull of institutional traditions; even ambitious ex ante proposals get reshaped by the US system's preference for ex post, risk-based approaches.

The SB~1047 debate catalysed a wave of state-level AI legislation in 2025, with direct consequences for cyber governance coherence. California enacted SB~53, the Transparency in Frontier Artificial Intelligence Act,\autocite{casb53_2025} on 29 September 2025, requiring developers of frontier models (trained at $>10^{26}$ operations) to disclose risk-governance plans, pre-deployment transparency reports, and critical safety incidents within 15 days. New York followed with the RAISE Act\autocite{ny_raise_2025} in December 2025, imposing annual independent audits and 72-hour incident reporting. For cyber governance, these state-level regimes create overlapping but non-identical compliance obligations, fragmenting the domestic regulatory environment in ways that complicate cross-border interoperability. A firm operating AI systems across both states faces two different incident-reporting timelines and audit standards, neither of which aligns with the EU AI Act's conformity-assessment framework or China's registration regime. The result is a domestic microcosm of the broader interoperability deficit the trilemma predicts.

% ──────────────────────────────────────────────
\section{Discussion: The Cyber Governance Trilemma}
% ──────────────────────────────────────────────

\subsection{Comparative Synthesis}

When the governance typology from Section~2 is applied to the three cases, the jurisdictions sort cleanly. The EU sits in the centralised, ex ante quadrant: the AI Act establishes a unified, risk-based regime that regulates AI systems \textit{before} they reach the market. A new centralised authority enforces it. At the opposite corner, the US relies on distributed, ex post oversight; even before the revocation of EO~\#14110, the federal approach depended heavily on industry self-regulation and existing legal structures. China falls between the two: centralised guidance and registration requirements coexist with decentralised, sector-specific enforcement and calculated regulatory forbearance for fast-growing SMEs.

These divergent positions produce what we term the \textit{cyber governance trilemma}. Borrowing from the international political economy literature on `impossible trinities', we argue that jurisdictions cannot simultaneously optimise three objectives:

\begin{enumerate}
  \item \textbf{Innovation speed}: minimising regulatory barriers to AI development and deployment.
  \item \textbf{Safety and rights protection}: ensuring AI systems do not cause harm, discrimination, or rights violations.
  \item \textbf{Regulatory interoperability}: maintaining compatibility with other jurisdictions' rules to enable cross-border operations.
\end{enumerate}

This is not coincidence; it reflects a structural incompatibility among the three objectives, operating through three bilateral tensions. Rodrik's globalisation trilemma provides the closest parallel, which holds that nations cannot simultaneously maintain deep economic integration, national sovereignty, and democratic politics.\autocite{rodrik2011globalization} Rodrik showed that the constraints are distributional: the gains from integration come at the cost of policy autonomy or democratic accountability. The AI governance trilemma works similarly. It is driven by distributional choices about who bears risk and who captures economic value, and each objective requires institutional commitments that undermine the other two.

Consider first the \textit{innovation--safety tension}, the most familiar of the three. Stringent ex ante regulation requires developers to demonstrate compliance before deployment, adding time and cost that slow innovation. The EU's conformity assessment regime for high-risk AI systems is a case in point: providers must complete risk assessments and establish quality management systems with full technical documentation before gaining market access, a process that delays deployment relative to jurisdictions without such requirements. Conversely, permissive regulation accelerates deployment but increases harm probability, as the US experience with largely unregulated AI-enabled content recommendation systems and deepfake generation tools demonstrates.

The \textit{safety--interoperability tension} gets less attention but binds just as tightly. Stronger domestic safety standards inevitably diverge from other jurisdictions' definitions of acceptable risk, impeding cross-border AI operations. China's content-labelling requirements for AI-generated material, for example, are fundamentally incompatible with EU transparency standards, which focus on system-level risk classification rather than content-level marking. The more rigorously a jurisdiction specifies its safety requirements, and the more those requirements reflect domestic political values, the wider the divergence from jurisdictions with different risk tolerances.

A third edge of the triangle, the \textit{innovation--interoperability tension}, operates through the Brussels Effect mechanism itself. Establishing global regulatory standards requires detailed, prescriptive rules with extraterritorial reach; but such rules constrain domestic innovators who must comply with standards designed for international projection. The EU's GPAI provisions have drawn criticism on exactly this point: they impose compliance burdens on European open-source AI developers that their US and Chinese competitors do not face, potentially deterring the very innovation that global standard-setting requires.

At bottom, the trilemma reflects distributional choices in AI governance: who bears risk, who captures value, and on whose terms norms are set. These choices map onto incompatible political economy preferences across jurisdictions. No single regulatory design can simultaneously minimise barriers to innovation and maximise safety guarantees without sacrificing cross-border compatibility, because each objective demands institutional choices that work against the other two. Each jurisdiction therefore sacrifices one objective, as illustrated in Figure~\ref{fig:trilemma} and summarised in Table~\ref{tab:trilemma}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/trilemma_triangle.pdf}
    \caption{The cyber governance trilemma. Each jurisdiction is positioned near the two objectives it prioritises, sacrificing the third. The three edges represent the bilateral tensions that drive the structural trade-off.}
    \label{fig:trilemma}
\end{figure} Safety and interoperability are the EU's twin priorities, pursued through Brussels Effect ambitions but at the cost of innovation velocity. The US, particularly after the revocation of EO~\#14110, has made the opposite bet: innovation speed first, with safety guarantees left largely to market discipline and state-level experimentation. China charts a third course, coupling rapid AI development with selective safety enforcement even as it embeds the entire apparatus within a cyber sovereignty strategy that is inherently incompatible with multi-stakeholder governance models.\autocite{mueller2017internet}

\begin{table}[ht]
\centering\small
\begin{tabularx}{\linewidth}{@{}l c c c@{}}
\toprule
\textbf{Jurisdiction} & \textbf{Innovation} & \textbf{Safety/Rights} & \textbf{Interoperability} \\
\midrule
European Union & Constrained & Strong (ex ante) & High (Brussels Effect) \\
United States & Prioritised & Weak (post-EO revocation) & Moderate \\
China & Prioritised (selective) & Selective enforcement & Low (cyber sovereignty) \\
\bottomrule
\end{tabularx}
\caption{The cyber governance trilemma: how each jurisdiction resolves the innovation--safety--interoperability trade-off.}
\label{tab:trilemma}
\end{table}

On the Brussels Effect thesis, our analysis offers only partial support. Extraterritorial reach does create compliance incentives for global firms, but the simultaneous emergence of substantive regulatory regimes in both the US and China limits the unilateral standard-setting power that the EU enjoyed with the GDPR. AI governance is evolving towards a \textit{multipolar regulatory order}\autocite{smuha2021race} in which firms must contend with competing compliance regimes, a pattern that mirrors the broader fragmentation of internet governance.\autocite{nye2014regime}

\subsection{Implications for International Cyber Governance}

Regulatory fragmentation carries concrete implications for the international cyber governance community.

Most immediately, divergence itself becomes a source of cyber risk. Divergent AI governance regimes create compliance gaps, seams between regulatory regimes that well-resourced threat actors can exploit. Where AI systems in critical infrastructure must satisfy different safety standards depending on jurisdiction, the weakest link determines overall cyber resilience. The EU's pairing of the AI Act with the NIS2 Directive illustrates one model of integrated AI-cybersecurity governance, but no comparable integration exists at the international level.

These gaps are widening. The Trump administration's revocation of EO~\#14110 opened a regulatory gulf between the US and EU at the moment the AI Act entered enforcement. European firms operating in the US face declining regulatory certainty, yet US firms must still comply with the AI Act for EU market access. Meanwhile, China's integration of AI governance into its Cybersecurity Law creates a self-contained regulatory system that reinforces cyber sovereignty, complicating already-fraught US--China technology competition and multilateral cyber norm processes.

Ripple effects extend well beyond the three major jurisdictions. Developing countries that lack the institutional capacity to design bespoke AI governance systems face a choice between regulatory models that carries geopolitical weight.\autocite{denardis2014global}\autocite{carr2016public} The GDPR experience, in which more than 130 countries adopted data protection laws influenced by the EU model, suggests that many countries will align with one of the three major AI regulatory approaches. Early evidence confirms this expectation but also reveals more complex dynamics than simple emulation.

Brazil's AI regulatory bill, approved by the Senate in December 2024, draws heavily on the EU AI Act's risk-based classification system and incorporates provisions for algorithmic impact assessments tailored to the country's acute concerns about racial and socioeconomic discrimination. India's proposed Digital India Act takes a different path, emphasising light-touch regulation and industry self-governance that more closely resembles the US approach, a choice driven by India's ambition to position itself as a global AI development hub. Singapore's Model AI Governance Framework, now in its third iteration, charts a pragmatic middle course: voluntary guidelines grounded in risk assessment and transparency principles, designed to attract international AI investment without imposing compliance burdens that would deter firms from establishing regional headquarters.

At the multilateral level, the African Union's Continental AI Strategy (2024) and the ASEAN Guide on AI Governance and Ethics both signal a preference for principles-based arrangements that preserve regulatory flexibility; but the practical governance choices of individual member states are increasingly shaped by which major power provides their AI infrastructure. Developing nations' choices about AI regulation determine their alignment in international cyber norm debates, including their positions in the UN Open-Ended Working Group on Information and Communications Technologies and the Global Digital Compact. The trilemma therefore operates across the entire global governance architecture, as regulatory choices cascade from investment relationships and infrastructure dependencies into multilateral alignments.

All of this complicates efforts to establish international AI governance norms through the United Nations and other multilateral forums. If the three largest AI powers cannot agree on basic regulatory approaches, whether ex ante or ex post, centralised or distributed, the prospects for a binding international regime remain limited. More realistic pathways may include issue-specific agreements on autonomous weapons or AI-enabled cyber operations, or mutual recognition arrangements that allow informal convergence through regulatory competition.

\subsection{AI Governance and Cyber Conflict}

The cyber governance trilemma extends beyond civilian AI regulation into the military and security domain, where all three bilateral tensions converge. AI-enabled capabilities are transforming cyber operations across all three jurisdictions: Automated vulnerability discovery and AI-generated spear-phishing at scale both blur the boundary between civilian AI governance and military cyber operations.\autocite{kello2017virtual}

Each jurisdiction handles this boundary differently, and none handles it well. In Europe, the AI Act explicitly exempts national security and defence applications from its scope, leaving military AI systems without a comparable governance regime at the EU level. China takes the opposite approach: its military-civil fusion strategy deliberately erases the distinction between civilian and military AI development, systematically channelling technologies developed under civilian regulations into People's Liberation Army modernisation programmes. The result is that China's ostensibly civilian AI governance apparatus has military implications that neither the EU nor the US accounts for. In the United States, programmes such as the Department of Defence's Replicator initiative and DARPA's AI-enabled cyber operations research are producing autonomous systems that occupy a governance grey zone between EO~14179's civilian innovation mandate and the Laws of Armed Conflict.

In the military domain, the trilemma bites hardest. No jurisdiction can simultaneously pursue AI-enabled military advantage (innovation), compliance with international humanitarian law and arms-control norms (safety), and interoperability with allies' autonomous systems (interoperability). The UN Group of Governmental Experts on Lethal Autonomous Weapons Systems and the Open-Ended Working Group on ICT security have both struggled to establish meaningful norms precisely because the three major AI powers approach military AI governance from incompatible positions. Issue-specific agreements (on autonomous cyber weapons or AI-generated disinformation in armed conflict) may represent the most achievable near-term pathway, but even these require a degree of regulatory convergence that the trilemma inherently impedes.

% ──────────────────────────────────────────────
\section{Conclusion}
% ──────────────────────────────────────────────

The EU, China, and the United States are constructing fundamentally different AI regulatory systems, shaped by distinct institutions, values, and geopolitical aims. Regulatory fragmentation of this kind is a persistent feature of cyber governance, not a transitional inconvenience; it affects both cybersecurity and international cooperation, with consequences for the distribution of technological power that extend well beyond the three jurisdictions examined here. As AI capabilities advance and become more deeply woven into critical systems, the pressures on all three vertices of the trilemma will grow. Whether jurisdictions can find workable compromises between innovation, safety, and interoperability will shape international cyber governance. There is little reason to expect convergence any time soon.

% ──────────────────────────────────────────────
\section*{Author Contributions}
% ──────────────────────────────────────────────
Jon Chun led the comparative analysis of the United States and China, the cyber governance trilemma framework, and the regulatory timeline dataset. Christian Schroeder de Witt led the analysis of the European Union and the regime complexity framing. Katherine Elkins contributed to the United States analysis and the developing-nations implications. All authors contributed to the theoretical framework and comparative synthesis.

% ──────────────────────────────────────────────
\section*{About the Authors}
% ──────────────────────────────────────────────

\textbf{Jon Chun} is Visiting Instructor of Humanities at Kenyon College. He holds graduate degrees in computer science and electrical engineering from UC Berkeley and UT Austin and spent over a decade in the cybersecurity and FinTech industries before entering academia. He leads the Modern Language Association's participation in the NIST US AI Safety Institute and is co-principal investigator on an IBM--Notre Dame Tech Ethics Lab grant studying AI decision-making in criminal justice. His research focuses on AI-cyber risk, AI safety, and the governance of emerging technologies.

\textbf{Christian Schroeder de Witt} is Lecturer in Engineering Science at the University of Oxford, where he leads the Oxford Witt Lab for Trust in AI (OWL). A Royal Academy of Engineering Research Fellow and Schmidt Sciences AI2050 Fellow, he works on multi-agent security and AI assurance, with a focus on securing decentralised AI systems against adversarial and systemic cyber threats. He advises RAND, the BBC, and the UK government on AI governance and cyber risk.

\textbf{Katherine Elkins} is Professor of Comparative Literature and Humanities at Kenyon College, with an affiliated appointment in Computing. She co-founded Kenyon's human-centred AI curriculum in 2016 and is co-principal investigator for the NIST US AI Safety Institute (through the MLA) and the IBM--Notre Dame Tech Ethics Lab grant on algorithmic decision-making in recidivism cases. Her research applies humanistic analysis to questions of AI safety, algorithmic bias, and the societal implications of cyber-enabled technologies.

% ──────────────────────────────────────────────
% Endnotes — Chatham House numbered endnotes
% ──────────────────────────────────────────────
\FloatBarrier
\clearpage
\begingroup
\parindent 0pt
\parskip 0.5\baselineskip
\theendnotes
\endgroup

\end{document}
